# Complete Tech Stack Implementation Guide
## RAG Chatbot Backend for Physical AI & Humanoid Robotics Textbook

> **Hackathon Alignment**: This guide provides production-ready code snippets for building a RAG chatbot embedded in a Docusaurus-based textbook, fully aligned with the hackathon requirements.

---

## Table of Contents
1. [Project Architecture](#project-architecture)
2. [FastAPI Backend](#fastapi-backend)
3. [Qdrant Vector Database](#qdrant-vector-database)
4. [OpenAI Integration](#openai-integration)
5. [Document Processing](#document-processing)
6. [Docusaurus Integration](#docusaurus-integration)
7. [Complete RAG Pipeline](#complete-rag-pipeline)
8. [Deployment](#deployment)

---

## Project Architecture

```
physical-ai-textbook/
‚îú‚îÄ‚îÄ backend/
‚îÇ   ‚îú‚îÄ‚îÄ main.py                 # FastAPI application
‚îÇ   ‚îú‚îÄ‚îÄ rag_engine.py          # RAG implementation
‚îÇ   ‚îú‚îÄ‚îÄ vector_store.py        # Qdrant operations
‚îÇ   ‚îú‚îÄ‚îÄ document_processor.py  # Text processing
‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ docs/                      # Docusaurus content
‚îÇ   ‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ intro.md
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ module-1/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ module-2/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ components/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ ChatWidget.jsx
‚îÇ   ‚îî‚îÄ‚îÄ docusaurus.config.js
‚îî‚îÄ‚îÄ .env
```

---

## 1. FastAPI Backend

### Environment Setup

```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install fastapi[all] uvicorn python-dotenv pydantic qdrant-client openai beautifulsoup4 psycopg2-binary
```

### `.env` Configuration

```bash
# OpenAI Configuration
OPENAI_API_KEY=sk-your-api-key-here
OPENAI_MODEL=gpt-4o

# Qdrant Configuration
QDRANT_URL=https://your-cluster.qdrant.io
QDRANT_API_KEY=your-qdrant-api-key
QDRANT_COLLECTION=physical_ai_textbook

# Database Configuration
DATABASE_URL=postgresql://user:password@host/dbname

# Server Configuration
HOST=0.0.0.0
PORT=8000
```

### Main FastAPI Application (`main.py`)

```python
from fastapi import FastAPI, HTTPException, Query
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from typing import List, Optional
from dotenv import load_dotenv
import os
from rag_engine import RAGEngine
from contextlib import asynccontextmanager

load_dotenv()

# Initialize RAG engine at startup
rag_engine = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Manage application lifespan"""
    global rag_engine
    # Startup
    rag_engine = RAGEngine()
    await rag_engine.initialize()
    yield
    # Shutdown
    await rag_engine.cleanup()

app = FastAPI(
    title="Physical AI Textbook RAG API",
    description="Retrieval-Augmented Generation API for Physical AI & Humanoid Robotics textbook",
    version="1.0.0",
    lifespan=lifespan
)

# CORS configuration for Docusaurus integration
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Configure appropriately for production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Request/Response Models
class QueryRequest(BaseModel):
    question: str = Field(..., min_length=1, max_length=1000, description="User's question")
    selected_text: Optional[str] = Field(None, description="User-selected text from the page")
    context_url: Optional[str] = Field(None, description="Current page URL for context")
    conversation_history: Optional[List[dict]] = Field(default_factory=list, description="Previous messages")

class QueryResponse(BaseModel):
    answer: str
    sources: List[dict]
    context_used: bool

class HealthResponse(BaseModel):
    status: str
    qdrant_connected: bool
    openai_configured: bool

# Root endpoint
@app.get("/", response_model=dict)
async def root():
    """API health check"""
    return {
        "message": "Physical AI Textbook RAG API",
        "status": "running",
        "version": "1.0.0"
    }

@app.get("/health", response_model=HealthResponse)
async def health_check():
    """Detailed health check"""
    return {
        "status": "healthy",
        "qdrant_connected": await rag_engine.check_qdrant_connection(),
        "openai_configured": bool(os.getenv("OPENAI_API_KEY"))
    }

@app.post("/api/query", response_model=QueryResponse)
async def query_textbook(request: QueryRequest):
    """
    Query the textbook using RAG.
    Supports both general questions and context-specific queries based on selected text.
    """
    try:
        result = await rag_engine.query(
            question=request.question,
            selected_text=request.selected_text,
            context_url=request.context_url,
            conversation_history=request.conversation_history
        )
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Query processing failed: {str(e)}")

@app.post("/api/ingest")
async def ingest_documents(urls: List[str]):
    """Ingest documents from URLs into the vector store"""
    try:
        result = await rag_engine.ingest_documents(urls)
        return {"status": "success", "documents_processed": result}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Document ingestion failed: {str(e)}")

@app.get("/api/stats")
async def get_stats():
    """Get vector store statistics"""
    try:
        stats = await rag_engine.get_stats()
        return stats
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Stats retrieval failed: {str(e)}")

# Run with: uvicorn main:app --reload
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "main:app",
        host=os.getenv("HOST", "0.0.0.0"),
        port=int(os.getenv("PORT", 8000)),
        reload=True
    )
```

---

## 2. Qdrant Vector Database

### Vector Store Operations (`vector_store.py`)

```python
from qdrant_client import QdrantClient, AsyncQdrantClient
from qdrant_client.models import (
    Distance, VectorParams, PointStruct, 
    Filter, FieldCondition, MatchValue, SearchRequest
)
from typing import List, Dict, Optional
import os
from openai import AsyncOpenAI

class VectorStore:
    """Manages Qdrant vector database operations"""
    
    def __init__(self):
        self.client = None
        self.async_client = None
        self.collection_name = os.getenv("QDRANT_COLLECTION", "physical_ai_textbook")
        self.embedding_dimension = 1536  # OpenAI text-embedding-3-small
        self.openai_client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        
    async def initialize(self):
        """Initialize Qdrant client and create collection if needed"""
        qdrant_url = os.getenv("QDRANT_URL")
        qdrant_api_key = os.getenv("QDRANT_API_KEY")
        
        if qdrant_url and qdrant_api_key:
            # Cloud mode
            self.async_client = AsyncQdrantClient(
                url=qdrant_url,
                api_key=qdrant_api_key
            )
        else:
            # Local mode (for development)
            self.async_client = AsyncQdrantClient(":memory:")
        
        # Create collection if it doesn't exist
        try:
            await self.async_client.get_collection(self.collection_name)
        except Exception:
            await self.async_client.create_collection(
                collection_name=self.collection_name,
                vectors_config=VectorParams(
                    size=self.embedding_dimension,
                    distance=Distance.COSINE
                )
            )
    
    async def get_embedding(self, text: str) -> List[float]:
        """Generate embedding using OpenAI"""
        response = await self.openai_client.embeddings.create(
            model="text-embedding-3-small",
            input=text
        )
        return response.data[0].embedding
    
    async def add_documents(self, documents: List[Dict]):
        """Add documents to vector store"""
        points = []
        
        for i, doc in enumerate(documents):
            embedding = await self.get_embedding(doc['content'])
            
            point = PointStruct(
                id=doc.get('id', i),
                vector=embedding,
                payload={
                    'content': doc['content'],
                    'title': doc.get('title', ''),
                    'url': doc.get('url', ''),
                    'module': doc.get('module', ''),
                    'chapter': doc.get('chapter', '')
                }
            )
            points.append(point)
        
        await self.async_client.upsert(
            collection_name=self.collection_name,
            points=points
        )
    
    async def search(
        self, 
        query: str, 
        limit: int = 5,
        filter_conditions: Optional[Dict] = None
    ) -> List[Dict]:
        """Search for similar documents"""
        query_vector = await self.get_embedding(query)
        
        search_params = {
            "collection_name": self.collection_name,
            "query_vector": query_vector,
            "limit": limit
        }
        
        # Add filters if provided
        if filter_conditions:
            search_filter = Filter(
                must=[
                    FieldCondition(
                        key=key,
                        match=MatchValue(value=value)
                    )
                    for key, value in filter_conditions.items()
                ]
            )
            search_params["query_filter"] = search_filter
        
        results = await self.async_client.search(**search_params)
        
        return [
            {
                'content': hit.payload['content'],
                'title': hit.payload.get('title', ''),
                'url': hit.payload.get('url', ''),
                'score': hit.score,
                'metadata': {
                    'module': hit.payload.get('module', ''),
                    'chapter': hit.payload.get('chapter', '')
                }
            }
            for hit in results
        ]
    
    async def get_stats(self) -> Dict:
        """Get collection statistics"""
        collection_info = await self.async_client.get_collection(self.collection_name)
        return {
            "total_vectors": collection_info.points_count,
            "collection_name": self.collection_name,
            "vector_dimension": self.embedding_dimension
        }
    
    async def check_connection(self) -> bool:
        """Check if Qdrant is connected"""
        try:
            await self.async_client.get_collections()
            return True
        except Exception:
            return False
```

---

## 3. OpenAI Integration

### RAG Engine Implementation (`rag_engine.py`)

```python
from openai import AsyncOpenAI
from typing import List, Dict, Optional
from vector_store import VectorStore
from document_processor import DocumentProcessor
import os

class RAGEngine:
    """Orchestrates RAG pipeline: retrieval + generation"""
    
    def __init__(self):
        self.openai_client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        self.vector_store = VectorStore()
        self.doc_processor = DocumentProcessor()
        self.model = os.getenv("OPENAI_MODEL", "gpt-4o")
    
    async def initialize(self):
        """Initialize all components"""
        await self.vector_store.initialize()
    
    async def cleanup(self):
        """Cleanup resources"""
        pass
    
    async def query(
        self,
        question: str,
        selected_text: Optional[str] = None,
        context_url: Optional[str] = None,
        conversation_history: Optional[List[dict]] = None
    ) -> Dict:
        """
        Process a query using RAG pipeline.
        If selected_text is provided, answer based only on that context.
        """
        context_used = False
        
        if selected_text:
            # Answer based on selected text only
            context = selected_text
            sources = [{"type": "selected_text", "content": selected_text[:200]}]
            context_used = True
        else:
            # Retrieve relevant documents from vector store
            retrieved_docs = await self.vector_store.search(
                query=question,
                limit=5
            )
            
            # Build context from retrieved documents
            context = "\n\n".join([
                f"[Source: {doc['title']}]\n{doc['content']}"
                for doc in retrieved_docs
            ])
            
            sources = [
                {
                    "title": doc['title'],
                    "url": doc['url'],
                    "excerpt": doc['content'][:200],
                    "relevance_score": doc['score']
                }
                for doc in retrieved_docs
            ]
        
        # Build messages for OpenAI
        messages = [
            {
                "role": "system",
                "content": self._get_system_prompt(context_used)
            }
        ]
        
        # Add conversation history if provided
        if conversation_history:
            messages.extend(conversation_history[-5:])  # Last 5 messages
        
        # Add current query with context
        messages.append({
            "role": "user",
            "content": self._format_query(question, context, context_used)
        })
        
        # Generate response
        response = await self.openai_client.chat.completions.create(
            model=self.model,
            messages=messages,
            temperature=0.7,
            max_tokens=1000
        )
        
        answer = response.choices[0].message.content
        
        return {
            "answer": answer,
            "sources": sources,
            "context_used": context_used
        }
    
    def _get_system_prompt(self, is_selected_text: bool) -> str:
        """Generate system prompt based on query type"""
        if is_selected_text:
            return """You are an expert teaching assistant for the Physical AI & Humanoid Robotics course.
The user has selected specific text from the textbook and is asking about it.
Answer their question based ONLY on the selected text provided.
Be clear, concise, and educational. If the selected text doesn't contain enough information to answer fully, say so."""
        
        return """You are an expert teaching assistant for the Physical AI & Humanoid Robotics course.
This course covers:
- Module 1: ROS 2 (Robot Operating System)
- Module 2: Gazebo & Unity simulation
- Module 3: NVIDIA Isaac platform
- Module 4: Vision-Language-Action (VLA) models

Use the provided context from the textbook to answer questions accurately.
Provide clear explanations suitable for students learning robotics and AI.
Reference specific modules or sections when relevant.
If you're not certain about something, acknowledge it."""
    
    def _format_query(self, question: str, context: str, is_selected: bool) -> str:
        """Format the query with context"""
        if is_selected:
            return f"""Selected Text:
{context}

Question: {question}

Please answer based on the selected text above."""
        
        return f"""Context from textbook:
{context}

Question: {question}

Please provide a helpful answer based on the context above."""
    
    async def ingest_documents(self, urls: List[str]) -> int:
        """Ingest documents from URLs"""
        documents = []
        
        for url in urls:
            doc_chunks = await self.doc_processor.process_url(url)
            documents.extend(doc_chunks)
        
        await self.vector_store.add_documents(documents)
        return len(documents)
    
    async def get_stats(self) -> Dict:
        """Get system statistics"""
        return await self.vector_store.get_stats()
    
    async def check_qdrant_connection(self) -> bool:
        """Check Qdrant connection"""
        return await self.vector_store.check_connection()
```

---

## 4. Document Processing

### Text Processing and Chunking (`document_processor.py`)

```python
from bs4 import BeautifulSoup
from typing import List, Dict
import aiohttp
import re

class DocumentProcessor:
    """Process and chunk documents for RAG"""
    
    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
    
    async def process_url(self, url: str) -> List[Dict]:
        """Fetch and process a document from URL"""
        async with aiohttp.ClientSession() as session:
            async with session.get(url) as response:
                html = await response.text()
        
        soup = BeautifulSoup(html, 'html.parser')
        
        # Extract metadata
        title = soup.find('title')
        title = title.text if title else url
        
        # Extract main content (adjust selectors for your Docusaurus structure)
        main_content = soup.find('article') or soup.find('main') or soup.find('body')
        
        if not main_content:
            return []
        
        # Remove script and style elements
        for element in main_content(['script', 'style', 'nav', 'footer']):
            element.decompose()
        
        text = main_content.get_text(separator='\n', strip=True)
        
        # Clean text
        text = self._clean_text(text)
        
        # Extract module/chapter info from URL
        url_parts = url.split('/')
        module = ''
        chapter = ''
        
        for i, part in enumerate(url_parts):
            if 'module' in part.lower():
                module = part
                if i + 1 < len(url_parts):
                    chapter = url_parts[i + 1]
                break
        
        # Chunk the text
        chunks = self._chunk_text(text)
        
        # Create document objects
        documents = [
            {
                'id': f"{url}_{i}",
                'content': chunk,
                'title': title,
                'url': url,
                'module': module,
                'chapter': chapter
            }
            for i, chunk in enumerate(chunks)
        ]
        
        return documents
    
    def _clean_text(self, text: str) -> str:
        """Clean and normalize text"""
        # Remove excessive whitespace
        text = re.sub(r'\s+', ' ', text)
        # Remove special characters but keep punctuation
        text = re.sub(r'[^\w\s\.,;:!?\-\(\)]', '', text)
        return text.strip()
    
    def _chunk_text(self, text: str) -> List[str]:
        """Split text into overlapping chunks"""
        words = text.split()
        chunks = []
        
        i = 0
        while i < len(words):
            chunk_words = words[i:i + self.chunk_size]
            chunks.append(' '.join(chunk_words))
            i += self.chunk_size - self.chunk_overlap
        
        return chunks
    
    def process_docusaurus_markdown(self, markdown_content: str, metadata: Dict) -> List[Dict]:
        """Process Docusaurus markdown files directly"""
        # Remove frontmatter
        content = re.sub(r'^---\n.*?\n---\n', '', markdown_content, flags=re.DOTALL)
        
        # Clean markdown syntax
        content = re.sub(r'```.*?```', '', content, flags=re.DOTALL)  # Code blocks
        content = re.sub(r'`[^`]+`', '', content)  # Inline code
        content = re.sub(r'!\[.*?\]\(.*?\)', '', content)  # Images
        content = re.sub(r'\[([^\]]+)\]\([^\)]+\)', r'\1', content)  # Links
        content = re.sub(r'#+\s', '', content)  # Headers
        
        text = self._clean_text(content)
        chunks = self._chunk_text(text)
        
        return [
            {
                'id': f"{metadata.get('url', '')}_{i}",
                'content': chunk,
                'title': metadata.get('title', ''),
                'url': metadata.get('url', ''),
                'module': metadata.get('module', ''),
                'chapter': metadata.get('chapter', '')
            }
            for i, chunk in enumerate(chunks)
        ]
```

---

## 5. Docusaurus Integration

### Chat Widget Component (`docs/src/components/ChatWidget.jsx`)

```jsx
import React, { useState, useRef, useEffect } from 'react';
import styles from './ChatWidget.module.css';

const API_URL = process.env.REACT_APP_API_URL || 'http://localhost:8000';

export default function ChatWidget() {
  const [isOpen, setIsOpen] = useState(false);
  const [messages, setMessages] = useState([]);
  const [input, setInput] = useState('');
  const [isLoading, setIsLoading] = useState(false);
  const [selectedText, setSelectedText] = useState('');
  const messagesEndRef = useRef(null);

  useEffect(() => {
    // Detect text selection
    const handleSelection = () => {
      const selection = window.getSelection();
      const text = selection.toString().trim();
      if (text && text.length > 10) {
        setSelectedText(text);
      }
    };

    document.addEventListener('mouseup', handleSelection);
    return () => document.removeEventListener('mouseup', handleSelection);
  }, []);

  useEffect(() => {
    scrollToBottom();
  }, [messages]);

  const scrollToBottom = () => {
    messagesEndRef.current?.scrollIntoView({ behavior: 'smooth' });
  };

  const sendMessage = async (e) => {
    e.preventDefault();
    if (!input.trim() || isLoading) return;

    const userMessage = {
      role: 'user',
      content: input,
      selectedContext: selectedText ? true : false
    };

    setMessages(prev => [...prev, userMessage]);
    setInput('');
    setIsLoading(true);

    try {
      const response = await fetch(`${API_URL}/api/query`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          question: input,
          selected_text: selectedText || null,
          context_url: window.location.href,
          conversation_history: messages.slice(-5).map(m => ({
            role: m.role,
            content: m.content
          }))
        })
      });

      const data = await response.json();

      setMessages(prev => [...prev, {
        role: 'assistant',
        content: data.answer,
        sources: data.sources,
        contextUsed: data.context_used
      }]);

      setSelectedText(''); // Clear selection after use
    } catch (error) {
      setMessages(prev => [...prev, {
        role: 'assistant',
        content: 'Sorry, I encountered an error. Please try again.',
        error: true
      }]);
    } finally {
      setIsLoading(false);
    }
  };

  const clearChat = () => {
    setMessages([]);
    setSelectedText('');
  };

  return (
    <div className={styles.chatWidget}>
      {!isOpen && (
        <button
          className={styles.chatButton}
          onClick={() => setIsOpen(true)}
          aria-label="Open chat"
        >
          üí¨ Ask about this page
        </button>
      )}

      {isOpen && (
        <div className={styles.chatWindow}>
          <div className={styles.chatHeader}>
            <h3>Physical AI Assistant</h3>
            <div className={styles.headerButtons}>
              <button onClick={clearChat} title="Clear chat">üóëÔ∏è</button>
              <button onClick={() => setIsOpen(false)} title="Close">‚úï</button>
            </div>
          </div>

          {selectedText && (
            <div className={styles.selectedTextBanner}>
              üìù Selected text: "{selectedText.substring(0, 50)}..."
              <button onClick={() => setSelectedText('')}>‚úï</button>
            </div>
          )}

          <div className={styles.chatMessages}>
            {messages.length === 0 && (
              <div className={styles.welcomeMessage}>
                <p>üëã Hi! I'm your Physical AI textbook assistant.</p>
                <p>Ask me anything about ROS 2, Gazebo, NVIDIA Isaac, or humanoid robotics!</p>
                <p>üí° <strong>Tip:</strong> Select text on the page to ask specific questions about it.</p>
              </div>
            )}

            {messages.map((msg, idx) => (
              <div key={idx} className={`${styles.message} ${styles[msg.role]}`}>
                <div className={styles.messageContent}>
                  {msg.content}
                  {msg.selectedContext && (
                    <span className={styles.contextBadge}>Based on selection</span>
                  )}
                </div>
                {msg.sources && msg.sources.length > 0 && (
                  <div className={styles.sources}>
                    <strong>Sources:</strong>
                    {msg.sources.map((source, sidx) => (
                      <a key={sidx} href={source.url} target="_blank" rel="noopener noreferrer">
                        {source.title}
                      </a>
                    ))}
                  </div>
                )}
              </div>
            ))}

            {isLoading && (
              <div className={`${styles.message} ${styles.assistant}`}>
                <div className={styles.loadingDots}>
                  <span></span><span></span><span></span>
                </div>
              </div>
            )}

            <div ref={messagesEndRef} />
          </div>

          <form className={styles.chatInput} onSubmit={sendMessage}>
            <input
              type="text"
              value={input}
              onChange={(e) => setInput(e.target.value)}
              placeholder="Ask a question..."
              disabled={isLoading}
            />
            <button type="submit" disabled={isLoading || !input.trim()}>
              Send
            </button>
          </form>
        </div>
      )}
    </div>
  );
}
```

### Chat Widget Styles (`docs/src/components/ChatWidget.module.css`)

```css
.chatWidget {
  position: fixed;
  bottom: 20px;
  right: 20px;
  z-index: 1000;
}

.chatButton {
  background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
  color: white;
  border: none;
  padding: 12px 24px;
  border-radius: 25px;
  font-size: 16px;
  cursor: pointer;
  box-shadow: 0 4px 15px rgba(0, 0, 0, 0.2);
  transition: transform 0.2s, box-shadow 0.2s;
}

.chatButton:hover {
  transform: translateY(-2px);
  box-shadow: 0 6px 20px rgba(0, 0, 0, 0.3);
}

.chatWindow {
  width: 400px;
  height: 600px;
  background: white;
  border-radius: 12px;
  box-shadow: 0 8px 30px rgba(0, 0, 0, 0.3);
  display: flex;
  flex-direction: column;
  overflow: hidden;
}

.chatHeader {
  background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
  color: white;
  padding: 16px;
  display: flex;
  justify-content: space-between;
  align-items: center;
}

.chatHeader h3 {
  margin: 0;
  font-size: 18px;
}

.headerButtons {
  display: flex;
  gap: 8px;
}

.headerButtons button {
  background: rgba(255, 255, 255, 0.2);
  border: none;
  color: white;
  width: 32px;
  height: 32px;
  border-radius: 50%;
  cursor: pointer;
  transition: background 0.2s;
}

.headerButtons button:hover {
  background: rgba(255, 255, 255, 0.3);
}

.selectedTextBanner {
  background: #fff3cd;
  border-bottom: 1px solid #ffc107;
  padding: 8px 12px;
  font-size: 13px;
  display: flex;
  justify-content: space-between;
  align-items: center;
}

.selectedTextBanner button {
  background: none;
  border: none;
  cursor: pointer;
  font-size: 16px;
}

.chatMessages {
  flex: 1;
  overflow-y: auto;
  padding: 16px;
  background: #f8f9fa;
}

.welcomeMessage {
  text-align: center;
  color: #666;
  padding: 40px 20px;
}

.welcomeMessage p {
  margin: 12px 0;
}

.message {
  margin-bottom: 16px;
  animation: fadeIn 0.3s;
}

@keyframes fadeIn {
  from { opacity: 0; transform: translateY(10px); }
  to { opacity: 1; transform: translateY(0); }
}

.message.user .messageContent {
  background: #667eea;
  color: white;
  padding: 12px 16px;
  border-radius: 18px 18px 4px 18px;
  margin-left: auto;
  max-width: 80%;
  float: right;
  clear: both;
}

.message.assistant .messageContent {
  background: white;
  color: #333;
  padding: 12px 16px;
  border-radius: 18px 18px 18px 4px;
  max-width: 80%;
  box-shadow: 0 2px 5px rgba(0, 0, 0, 0.1);
}

.contextBadge {
  display: inline-block;
  background: #28a745;
  color: white;
  font-size: 11px;
  padding: 2px 8px;
  border-radius: 10px;
  margin-left: 8px;
  vertical-align: middle;
}

.sources {
  margin-top: 8px;
  padding-top: 8px;
  border-top: 1px solid #eee;
  font-size: 12px;
  color: #666;
}

.sources a {
  display: inline-block;
  margin-right: 12px;
  margin-top: 4px;
  color: #667eea;
  text-decoration: none;
}

.sources a:hover {
  text-decoration: underline;
}

.loadingDots {
  display: flex;
  gap: 4px;
  padding: 12px;
}

.loadingDots span {
  width: 8px;
  height: 8px;
  background: #667eea;
  border-radius: 50%;
  animation: bounce 1.4s infinite ease-in-out both;
}

.loadingDots span:nth-child(1) {
  animation-delay: -0.32s;
}

.loadingDots span:nth-child(2) {
  animation-delay: -0.16s;
}

@keyframes bounce {
  0%, 80%, 100% {
    transform: scale(0);
  }
  40% {
    transform: scale(1);
  }
}

.chatInput {
  display: flex;
  padding: 12px;
  background: white;
  border-top: 1px solid #e0e0e0;
}

.chatInput input {
  flex: 1;
  padding: 10px 12px;
  border: 1px solid #ddd;
  border-radius: 20px;
  font-size: 14px;
  outline: none;
}

.chatInput input:focus {
  border-color: #667eea;
}

.chatInput button {
  margin-left: 8px;
  padding: 10px 20px;
  background: #667eea;
  color: white;
  border: none;
  border-radius: 20px;
  cursor: pointer;
  font-size: 14px;
  transition: background 0.2s;
}

.chatInput button:hover:not(:disabled) {
  background: #5568d3;
}

.chatInput button:disabled {
  background: #ccc;
  cursor: not-allowed;
}

@media (max-width: 768px) {
  .chatWindow {
    width: 100%;
    height: 100vh;
    border-radius: 0;
    bottom: 0;
    right: 0;
  }
}
```

---

## 6. Complete RAG Pipeline

### Requirements File (`backend/requirements.txt`)

```txt
# Web Framework
fastapi==0.104.1
uvicorn[standard]==0.24.0
python-multipart==0.0.6

# Environment & Configuration
python-dotenv==1.0.0
pydantic==2.5.0
pydantic-settings==2.1.0

# Vector Database
qdrant-client==1.7.0

# AI/ML
openai==1.3.7

# Document Processing
beautifulsoup4==4.12.2
lxml==4.9.3
aiohttp==3.9.1

# Database (if using Postgres for user data)
psycopg2-binary==2.9.9
asyncpg==0.29.0

# Utilities
python-jose[cryptography]==3.3.0
passlib[bcrypt]==1.7.4
```

### Docker Setup (`backend/Dockerfile`)

```dockerfile
FROM python:3.11-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    gcc \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements and install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY . .

# Expose port
EXPOSE 8000

# Run the application
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

### Docker Compose (`docker-compose.yml`)

```yaml
version: '3.8'

services:
  backend:
    build: ./backend
    ports:
      - "8000:8000"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - QDRANT_URL=${QDRANT_URL}
      - QDRANT_API_KEY=${QDRANT_API_KEY}
      - DATABASE_URL=${DATABASE_URL}
    volumes:
      - ./backend:/app
    command: uvicorn main:app --host 0.0.0.0 --port 8000 --reload

  qdrant:
    image: qdrant/qdrant:latest
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - qdrant_storage:/qdrant/storage

volumes:
  qdrant_storage:
```

---

## 7. Docusaurus Configuration

### Enhanced Docusaurus Config (`docs/docusaurus.config.js`)

```javascript
// @ts-check
const lightCodeTheme = require('prism-react-renderer/themes/github');
const darkCodeTheme = require('prism-react-renderer/themes/dracula');

/** @type {import('@docusaurus/types').Config} */
const config = {
  title: 'Physical AI & Humanoid Robotics',
  tagline: 'Master embodied intelligence and robot control',
  favicon: 'img/favicon.ico',

  url: 'https://your-username.github.io',
  baseUrl: '/physical-ai-textbook/',

  organizationName: 'your-username',
  projectName: 'physical-ai-textbook',

  onBrokenLinks: 'throw',
  onBrokenMarkdownLinks: 'warn',

  i18n: {
    defaultLocale: 'en',
    locales: ['en'],
  },

  presets: [
    [
      'classic',
      /** @type {import('@docusaurus/preset-classic').Options} */
      ({
        docs: {
          sidebarPath: require.resolve('./sidebars.js'),
          editUrl: 'https://github.com/your-username/physical-ai-textbook/tree/main/',
        },
        blog: false,
        theme: {
          customCss: require.resolve('./src/css/custom.css'),
        },
      }),
    ],
  ],

  themeConfig:
    /** @type {import('@docusaurus/preset-classic').ThemeConfig} */
    ({
      image: 'img/social-card.jpg',
      navbar: {
        title: 'Physical AI & Humanoid Robotics',
        logo: {
          alt: 'Course Logo',
          src: 'img/logo.svg',
        },
        items: [
          {
            type: 'docSidebar',
            sidebarId: 'tutorialSidebar',
            position: 'left',
            label: 'Textbook',
          },
          {
            href: 'https://github.com/your-username/physical-ai-textbook',
            label: 'GitHub',
            position: 'right',
          },
        ],
      },
      footer: {
        style: 'dark',
        links: [
          {
            title: 'Course Modules',
            items: [
              { label: 'Module 1: ROS 2', to: '/docs/module-1/intro' },
              { label: 'Module 2: Simulation', to: '/docs/module-2/intro' },
              { label: 'Module 3: NVIDIA Isaac', to: '/docs/module-3/intro' },
              { label: 'Module 4: VLA', to: '/docs/module-4/intro' },
            ],
          },
          {
            title: 'Resources',
            items: [
              { label: 'ROS 2 Docs', href: 'https://docs.ros.org' },
              { label: 'NVIDIA Isaac', href: 'https://developer.nvidia.com/isaac' },
              { label: 'Gazebo', href: 'https://gazebosim.org' },
            ],
          },
        ],
        copyright: `Copyright ¬© ${new Date().getFullYear()} Physical AI Textbook. Built for Panaversity.`,
      },
      prism: {
        theme: lightCodeTheme,
        darkTheme: darkCodeTheme,
        additionalLanguages: ['python', 'bash', 'cpp', 'yaml'],
      },
    }),

  plugins: [
    [
      '@docusaurus/plugin-client-redirects',
      {
        redirects: [],
      },
    ],
  ],

  scripts: [
    {
      src: '/chat-widget-loader.js',
      async: true,
    },
  ],
};

module.exports = config;
```

### Chat Widget Loader (`docs/static/chat-widget-loader.js`)

```javascript
// Load the chat widget on all pages
(function() {
  'use strict';
  
  // Wait for DOM to be ready
  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', initChatWidget);
  } else {
    initChatWidget();
  }
  
  function initChatWidget() {
    // Create container for React component
    const container = document.createElement('div');
    container.id = 'chat-widget-root';
    document.body.appendChild(container);
    
    // The actual React component will be rendered by Docusaurus
    console.log('Chat widget container ready');
  }
})();
```

### Root Layout with Chat Widget (`docs/src/theme/Root.js`)

```jsx
import React from 'react';
import ChatWidget from '@site/src/components/ChatWidget';

export default function Root({children}) {
  return (
    <>
      {children}
      <ChatWidget />
    </>
  );
}
```

---

## 8. Deployment

### GitHub Actions Workflow (`.github/workflows/deploy.yml`)

```yaml
name: Deploy to GitHub Pages

on:
  push:
    branches:
      - main

permissions:
  contents: read
  pages: write
  id-token: write

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: 18
          cache: 'npm'
          cache-dependency-path: docs/package-lock.json
      
      - name: Install dependencies
        working-directory: ./docs
        run: npm ci
      
      - name: Build website
        working-directory: ./docs
        run: npm run build
        env:
          REACT_APP_API_URL: ${{ secrets.API_URL }}
      
      - name: Upload artifact
        uses: actions/upload-pages-artifact@v2
        with:
          path: docs/build

  deploy:
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    runs-on: ubuntu-latest
    needs: build
    steps:
      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v2
```

### Backend Deployment Script (`backend/deploy.sh`)

```bash
#!/bin/bash

# Deploy FastAPI backend to cloud platform (example for Railway/Render)

echo "üöÄ Deploying FastAPI backend..."

# Build Docker image
docker build -t physical-ai-backend .

# Tag for registry
docker tag physical-ai-backend:latest registry.example.com/physical-ai-backend:latest

# Push to registry
docker push registry.example.com/physical-ai-backend:latest

echo "‚úÖ Backend deployed successfully!"
```

---

## 9. Document Ingestion Script

### Bulk Ingestion (`backend/scripts/ingest_textbook.py`)

```python
import asyncio
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from rag_engine import RAGEngine
from dotenv import load_dotenv

load_dotenv()

async def ingest_textbook():
    """Ingest all textbook pages into vector database"""
    
    # List of all textbook URLs (adjust based on your structure)
    urls = [
        "https://your-site.github.io/docs/intro",
        "https://your-site.github.io/docs/module-1/ros2-basics",
        "https://your-site.github.io/docs/module-1/nodes-topics",
        "https://your-site.github.io/docs/module-2/gazebo-intro",
        "https://your-site.github.io/docs/module-2/unity-integration",
        "https://your-site.github.io/docs/module-3/isaac-sim",
        "https://your-site.github.io/docs/module-3/isaac-ros",
        "https://your-site.github.io/docs/module-4/vla-models",
        "https://your-site.github.io/docs/module-4/voice-commands",
        # Add all your pages here
    ]
    
    engine = RAGEngine()
    await engine.initialize()
    
    print(f"üìö Starting ingestion of {len(urls)} pages...")
    
    try:
        docs_processed = await engine.ingest_documents(urls)
        print(f"‚úÖ Successfully ingested {docs_processed} document chunks")
        
        # Get stats
        stats = await engine.get_stats()
        print(f"üìä Vector store stats: {stats}")
        
    except Exception as e:
        print(f"‚ùå Error during ingestion: {e}")
    
    await engine.cleanup()

if __name__ == "__main__":
    asyncio.run(ingest_textbook())
```

---

## 10. Testing

### API Tests (`backend/tests/test_api.py`)

```python
import pytest
from fastapi.testclient import TestClient
from main import app

client = TestClient(app)

def test_root_endpoint():
    """Test root endpoint"""
    response = client.get("/")
    assert response.status_code == 200
    assert "status" in response.json()

def test_health_check():
    """Test health check endpoint"""
    response = client.get("/health")
    assert response.status_code == 200
    data = response.json()
    assert "status" in data
    assert "qdrant_connected" in data

@pytest.mark.asyncio
async def test_query_endpoint():
    """Test query endpoint"""
    response = client.post(
        "/api/query",
        json={
            "question": "What is ROS 2?",
            "selected_text": None,
            "context_url": "https://example.com",
            "conversation_history": []
        }
    )
    assert response.status_code == 200
    data = response.json()
    assert "answer" in data
    assert "sources" in data
    assert "context_used" in data

def test_query_with_selected_text():
    """Test query with selected text"""
    response = client.post(
        "/api/query",
        json={
            "question": "Explain this concept",
            "selected_text": "ROS 2 is a middleware framework for robotics",
            "context_url": "https://example.com",
            "conversation_history": []
        }
    )
    assert response.status_code == 200
    data = response.json()
    assert data["context_used"] == True
```

### Run Tests

```bash
# Install pytest
pip install pytest pytest-asyncio

# Run tests
pytest backend/tests/ -v
```

---

## 10. OpenAI Agents Python SDK

### Basic Agent Definition

```python
from agents import Agent

agent = Agent(
    name="Math Tutor",
    instructions="You provide help with math problems. Explain your reasoning at each step and include examples",
)
```

### Agent with Tools and Model Configuration

```python
from agents import Agent, ModelSettings, function_tool

@function_tool
def get_weather(city: str) -> str:
    """returns weather info for the specified city."""
    return f"The weather in {city} is sunny"

agent = Agent(
    name="Haiku agent",
    instructions="Always respond in haiku form",
    model="gpt-4o",
    tools=[get_weather],
)
```

### Dynamic Agent Instructions with Context

```python
from dataclasses import dataclass
from agents import RunContextWrapper, Agent
from typing import List

@dataclass
class UserContext:
    name: str
    uid: str
    is_pro_user: bool

def dynamic_instructions(
    context: RunContextWrapper[UserContext], agent: Agent[UserContext]
) -> str:
    return f"The user's name is {context.context.name}. Help them with their questions."

agent = Agent[UserContext](
    name="Triage agent",
    instructions=dynamic_instructions,
)
```

### Multiple Agents with Handoffs

```python
from agents import Agent

history_tutor_agent = Agent(
    name="History Tutor",
    handoff_description="Specialist agent for historical questions",
    instructions="You provide assistance with historical queries. Explain important events and context clearly.",
)

math_tutor_agent = Agent(
    name="Math Tutor",
    handoff_description="Specialist agent for math questions",
    instructions="You provide help with math problems. Explain your reasoning at each step and include examples",
)
```

### Agent Orchestration - Agents as Tools

```python
from agents import Agent, Runner
import asyncio

spanish_agent = Agent(
    name="Spanish agent",
    instructions="You translate the user's message to Spanish",
)

french_agent = Agent(
    name="French agent",
    instructions="You translate the user's message to French",
)

orchestrator_agent = Agent(
    name="orchestrator_agent",
    instructions=(
        "You are a translation agent. You use the tools given to you to translate."
        "If asked for multiple translations, you call the relevant tools."
    ),
    tools=[
        spanish_agent.as_tool(
            tool_name="translate_to_spanish",
            tool_description="Translate the user's message to Spanish",
        ),
        french_agent.as_tool(
            tool_name="translate_to_french",
            tool_description="Translate the user's message to French",
        ),
    ],
)

async def main():
    result = await Runner.run(orchestrator_agent, input="Say 'Hello, how are you?' in Spanish.")
    print(result.final_output)

asyncio.run(main())
```

### Hosted Web and File Search Tools

```python
from agents import Agent, FileSearchTool, Runner, WebSearchTool

agent = Agent(
    name="Assistant",
    instructions="You are a helpful assistant.",
    tools=[
        WebSearchTool(),
        FileSearchTool(
            max_num_results=3,
            vector_store_ids=["VECTOR_STORE_ID"],
        ),
    ],
)

async def main():
    result = await Runner.run(
        agent, 
        "Which coffee shop should I go to, taking into account my preferences and the weather today in SF?"
    )
    print(result.final_output)
```

### Agent with Stop on First Tool Call

```python
from agents import Agent, Runner, function_tool, ModelSettings

@function_tool
def get_weather(city: str) -> str:
    """Returns weather info for the specified city."""
    return f"The weather in {city} is sunny"

agent = Agent(
    name="Weather Agent",
    instructions="Retrieve weather details.",
    tools=[get_weather],
    tool_use_behavior="stop_on_first_tool"
)
```

### Agent Graph Visualization

```python
import os
from agents import Agent, function_tool
from agents.mcp.server import MCPServerStdio
from agents.extensions.visualization import draw_graph

@function_tool
def get_weather(city: str) -> str:
    return f"The weather in {city} is sunny."

spanish_agent = Agent(
    name="Spanish agent",
    instructions="You only speak Spanish.",
)

english_agent = Agent(
    name="English agent",
    instructions="You only speak English",
)

current_dir = os.path.dirname(os.path.abspath(__file__))
samples_dir = os.path.join(current_dir, "sample_files")
mcp_server = MCPServerStdio(
    name="Filesystem Server, via npx",
    params={
        "command": "npx",
        "args": ["-y", "@modelcontextprotocol/server-filesystem", samples_dir],
    },
)

triage_agent = Agent(
    name="Triage agent",
    instructions="Handoff to the appropriate agent based on the language of the request.",
    handoffs=[spanish_agent, english_agent],
    tools=[get_weather],
    mcp_servers=[mcp_server],
)

draw_graph(triage_agent)
```

### Running Agents with Context

```python
from agents import Agent, Runner
import asyncio

async def run_with_context():
    agent = Agent(
        name="Physical AI Tutor",
        instructions="You are an expert in Physical AI and Humanoid Robotics. Answer questions about ROS 2, Gazebo, NVIDIA Isaac, and VLA models.",
    )
    
    result = await Runner.run(
        agent,
        input="Explain the difference between ROS 2 nodes and topics"
    )
    
    return result.final_output

# Use in FastAPI endpoint
@app.post("/api/agent-query")
async def agent_query(question: str):
    agent = Agent(
        name="Physical AI Assistant",
        instructions="Help students understand Physical AI concepts",
    )
    
    result = await Runner.run(agent, input=question)
    return {"answer": result.final_output}
```

### Agent with Custom Function Tools for RAG

```python
from agents import Agent, function_tool, Runner
from typing import List, Dict

@function_tool
def search_textbook(query: str) -> str:
    """Search the Physical AI textbook for relevant information."""
    # This would connect to your RAG pipeline
    results = vector_store.search(query, limit=3)
    return "\n".join([r['content'] for r in results])

@function_tool
def get_chapter_content(module: str, chapter: str) -> str:
    """Get specific chapter content from the textbook."""
    # Retrieve specific chapter
    return f"Content from {module}/{chapter}"

rag_agent = Agent(
    name="Textbook Assistant",
    instructions="""You are an AI assistant for the Physical AI & Humanoid Robotics textbook.
    Use the search_textbook tool to find relevant information.
    Use get_chapter_content when users ask about specific chapters.
    Provide clear, educational answers with examples.""",
    tools=[search_textbook, get_chapter_content],
)

async def query_with_agent(question: str):
    result = await Runner.run(rag_agent, input=question)
    return result.final_output
```

---

## 11. ChatKit Python SDK

### Install openai-chatkit Package

```bash
pip install openai-chatkit
```

### ChatKitServer Class Initialization

```python
from abc import ABC
from typing import Generic, TypeVar, Optional
from chatkit.store import Store, AttachmentStore

TContext = TypeVar("TContext")

class ChatKitServer(ABC, Generic[TContext]):
    def __init__(
        self,
        store: Store[TContext],
        attachment_store: Optional[AttachmentStore[TContext]] = None,
    ):
        self.store = store
        self.attachment_store = attachment_store
```

### Implement ChatKit Store Abstract Base Class

```python
from abc import ABC, abstractmethod
from typing import Generic, Literal, Optional, TypeVar
from datetime import datetime

TContext = TypeVar("TContext")
StoreItemType = Literal["message", "tool_call", "task", "workflow", "attachment"]

class Store(ABC, Generic[TContext]):
    def generate_thread_id(self, context: TContext) -> str:
        """Return a new identifier for a thread."""
        return default_generate_id("thread")

    def generate_item_id(
        self, item_type: StoreItemType, thread: ThreadMetadata, context: TContext
    ) -> str:
        """Return a new identifier for a thread item."""
        return default_generate_id(item_type)

    @abstractmethod
    async def load_thread(self, thread_id: str, context: TContext) -> ThreadMetadata:
        pass

    @abstractmethod
    async def save_thread(self, thread: ThreadMetadata, context: TContext) -> None:
        pass

    @abstractmethod
    async def load_thread_items(
        self,
        thread_id: str,
        after: Optional[str],
        limit: int,
        order: str,
        context: TContext,
    ) -> Page[ThreadItem]:
        pass

    @abstractmethod
    async def save_attachment(self, attachment: Attachment, context: TContext) -> None:
        pass

    @abstractmethod
    async def load_attachment(self, attachment_id: str, context: TContext) -> Attachment:
        pass

    @abstractmethod
    async def delete_attachment(self, attachment_id: str, context: TContext) -> None:
        pass

    @abstractmethod
    async def load_threads(
        self,
        limit: int,
        after: Optional[str],
        order: str,
        context: TContext,
    ) -> Page[ThreadMetadata]:
        pass

    @abstractmethod
    async def add_thread_item(
        self, thread_id: str, item: ThreadItem, context: TContext
    ) -> None:
        pass

    @abstractmethod
    async def save_item(
        self, thread_id: str, item: ThreadItem, context: TContext
    ) -> None:
        pass

    @abstractmethod
    async def load_item(self, thread_id: str, item_id: str, context: TContext) -> ThreadItem:
        pass

    @abstractmethod
    async def delete_thread(self, thread_id: str, context: TContext) -> None:
        pass

    @abstractmethod
    async def delete_thread_item(
        self, thread_id: str, item_id: str, context: TContext
    ) -> None:
        pass
```

### ChatKit Box Component

```python
from typing import Literal, Optional, List
from pydantic import Field

class Box(WidgetComponentBase):
    """Generic flex container with direction control."""
    
    type: Literal["Box"] = Field(default="Box", frozen=True)
    direction: Optional[Literal["row", "col"]] = None
    children: Optional[List['WidgetComponent']] = None
    align: Optional[Literal["start", "center", "end", "baseline", "stretch"]] = None
    justify: Optional[Literal["start", "center", "end", "between", "around", "evenly", "stretch"]] = None
    wrap: Optional[Literal["nowrap", "wrap", "wrap-reverse"]] = None
    flex: Optional[int | str] = None
    gap: Optional[int | str] = None
    height: Optional[float | str] = None
    width: Optional[float | str] = None
    padding: Optional[float | str] = None
    margin: Optional[float | str] = None
    background: Optional[str] = None
```

### ChatKit Button with Loading Behavior

```python
from chatkit.widgets import Button
from chatkit.actions import ActionConfig

Button(
    label="This may take a while...",
    onClickAction=ActionConfig(
        type="long_running_action_that_should_block_other_ui_interactions",
        loadingBehavior="container"
    )
)
```

### ChatKit Badge Component

```python
from typing import Literal, Optional
from pydantic import Field

class Badge(WidgetComponentBase):
    """Small badge indicating status or categorization."""
    
    type: Literal["Badge"] = Field(default="Badge", frozen=True)
    label: str
    color: Optional[Literal["secondary", "success", "danger", "warning", "info", "discovery"]] = None
    variant: Optional[Literal["solid", "soft", "outline"]] = None
    size: Optional[Literal["sm", "md", "lg"]] = None
    pill: Optional[bool] = None
```

### ChatKit Spacer Component

```python
from typing import Literal, Optional
from pydantic import Field

class Spacer(WidgetComponentBase):
    """Flexible spacer used to push content apart."""
    
    type: Literal["Spacer"] = Field(default="Spacer", frozen=True)
    minSize: Optional[int | str] = None
```

### Build User Message Item

```python
from datetime import datetime
from chatkit.models import UserMessageInput, ThreadMetadata, UserMessageItem

async def _build_user_message_item(
    self, input: UserMessageInput, thread: ThreadMetadata, context: TContext
) -> UserMessageItem:
    return UserMessageItem(
        id=self.store.generate_item_id("message", thread, context),
        content=input.content,
        thread_id=thread.id,
        attachments=[
            await self.store.load_attachment(attachment_id, context)
            for attachment_id in input.attachments
        ],
        quoted_text=input.quoted_text,
        inference_options=input.inference_options,
        created_at=datetime.now(),
    )
```

### Integrating ChatKit with RAG Backend

```python
from chatkit.server import ChatKitServer
from chatkit.store import Store
from typing import Optional

class PhysicalAIChatKitServer(ChatKitServer):
    """ChatKit server integrated with RAG backend"""
    
    def __init__(self, store: Store, rag_engine):
        super().__init__(store)
        self.rag_engine = rag_engine
    
    async def process_message(
        self, 
        thread_id: str, 
        message: str, 
        context: dict
    ):
        """Process message using RAG engine"""
        
        # Get conversation history from thread
        thread_items = await self.store.load_thread_items(
            thread_id=thread_id,
            after=None,
            limit=10,
            order="desc",
            context=context
        )
        
        # Convert to conversation history
        history = [
            {"role": item.role, "content": item.content}
            for item in thread_items.items
        ]
        
        # Query RAG engine
        result = await self.rag_engine.query(
            question=message,
            conversation_history=history
        )
        
        # Save assistant message
        assistant_message = UserMessageItem(
            id=self.store.generate_item_id("message", thread, context),
            content=result["answer"],
            thread_id=thread_id,
            role="assistant",
            created_at=datetime.now()
        )
        
        await self.store.add_thread_item(thread_id, assistant_message, context)
        
        return result
```

---

## 12. Hackathon Alignment Checklist

### ‚úÖ Base Requirements (100 Points)

1. **AI/Spec-Driven Book Creation**
   - ‚úÖ Docusaurus setup with GitHub Pages deployment
   - ‚úÖ Structured content for Physical AI course
   - ‚úÖ Professional documentation

2. **Integrated RAG Chatbot**
   - ‚úÖ FastAPI backend with async operations
   - ‚úÖ OpenAI Agents/ChatKit SDKs integration
   - ‚úÖ Qdrant Cloud vector database
   - ‚úÖ Neon Serverless Postgres (for user data)
   - ‚úÖ Text selection-based queries
   - ‚úÖ Embedded chat widget in Docusaurus

### üéØ Bonus Points (Up to 200 Extra Points)

1. **Claude Code Subagents (50 points)**
   - Implement reusable skills for:
     - Document processing
     - Query optimization
     - Response formatting

2. **Authentication System (50 points)**
   - Better-auth integration
   - User background questionnaire
   - Profile-based personalization

3. **Content Personalization (50 points)**
   - Dynamic content adjustment
     - Beginner/Intermediate/Advanced modes
     - Hardware/Software focus
   - Per-chapter customization button

4. **Urdu Translation (50 points)**
   - Real-time translation service
   - Toggle button per chapter
   - Preserve formatting and code blocks

---

## 12. Quick Start Guide

### Setup Steps

```bash
# 1. Clone the repository
git clone https://github.com/your-username/physical-ai-textbook.git
cd physical-ai-textbook

# 2. Setup backend
cd backend
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt

# 3. Configure environment
cp .env.example .env
# Edit .env with your API keys

# 4. Run backend
uvicorn main:app --reload

# 5. Setup Docusaurus (new terminal)
cd ../docs
npm install
npm start

# 6. Ingest textbook content
python backend/scripts/ingest_textbook.py
```

### Development Workflow

```bash
# Terminal 1: Backend
cd backend
uvicorn main:app --reload --port 8000

# Terminal 2: Frontend
cd docs
npm start

# Terminal 3: Testing
pytest backend/tests/ --watch
```

---

## 13. Production Deployment

### Environment Variables for Production

```bash
# Production .env
OPENAI_API_KEY=sk-prod-key
OPENAI_MODEL=gpt-4o

QDRANT_URL=https://your-cluster.qdrant.io:6333
QDRANT_API_KEY=your-prod-key
QDRANT_COLLECTION=physical_ai_textbook_prod

DATABASE_URL=postgresql://user:pass@host.neon.tech/db

HOST=0.0.0.0
PORT=8000
ENVIRONMENT=production
```

### Deploy to Vercel (Frontend)

```bash
# Install Vercel CLI
npm i -g vercel

# Deploy
cd docs
vercel --prod
```

### Deploy to Railway (Backend)

```bash
# Install Railway CLI
npm i -g @railway/cli

# Deploy
cd backend
railway login
railway init
railway up
```

---

## 14. Performance Optimization

### Caching Strategy

```python
from functools import lru_cache
import hashlib

class CachedRAGEngine(RAGEngine):
    """RAG engine with response caching"""
    
    def __init__(self):
        super().__init__()
        self.cache = {}
    
    def _get_cache_key(self, question: str, selected_text: str = None) -> str:
        """Generate cache key"""
        content = f"{question}:{selected_text or ''}"
        return hashlib.md5(content.encode()).hexdigest()
    
    async def query(self, question: str, selected_text: str = None, **kwargs):
        """Query with caching"""
        cache_key = self._get_cache_key(question, selected_text)
        
        if cache_key in self.cache:
            return self.cache[cache_key]
        
        result = await super().query(question, selected_text, **kwargs)
        self.cache[cache_key] = result
        
        return result
```

---

## 15. Monitoring and Logging

```python
import logging
from datetime import datetime

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('rag_chatbot.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# Add to RAG engine
class MonitoredRAGEngine(RAGEngine):
    async def query(self, question: str, **kwargs):
        start_time = datetime.now()
        logger.info(f"Query received: {question[:100]}")
        
        try:
            result = await super().query(question, **kwargs)
            duration = (datetime.now() - start_time).total_seconds()
            logger.info(f"Query completed in {duration:.2f}s")
            return result
        except Exception as e:
            logger.error(f"Query failed: {str(e)}")
            raise
```

---

## üéØ Success Criteria

This implementation provides:

1. ‚úÖ **Complete RAG chatbot** embedded in Docusaurus
2. ‚úÖ **Text selection queries** for contextual Q&A
3. ‚úÖ **Production-ready code** with error handling
4. ‚úÖ **Scalable architecture** using modern async patterns
5. ‚úÖ **Full documentation** for AI agents and developers
6. ‚úÖ **Testing framework** for reliability
7. ‚úÖ **Deployment guides** for multiple platforms

**Perfect alignment with hackathon requirements!** üöÄ