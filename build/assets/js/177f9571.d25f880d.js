"use strict";(globalThis.webpackChunkphysical_ai_textbook=globalThis.webpackChunkphysical_ai_textbook||[]).push([[938],{3(e,n,r){r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"module1/week2-intro-physical-ai-2","title":"Sensors and Embodiment","description":"Understanding robot sensor systems - LiDAR, cameras, IMUs, and force sensors for Physical AI","source":"@site/docs/module1/week2-intro-physical-ai-2.md","sourceDirName":"module1","slug":"/module1/week2-intro-physical-ai-2","permalink":"/physical-ai-and-humanoid-robotics/docs/module1/week2-intro-physical-ai-2","draft":false,"unlisted":false,"editUrl":"https://github.com/nimranaz148/docs/module1/week2-intro-physical-ai-2.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"title":"Sensors and Embodiment","description":"Understanding robot sensor systems - LiDAR, cameras, IMUs, and force sensors for Physical AI","keywords":["sensors","lidar","imu","camera","force-sensor","embodiment"]},"sidebar":"tutorialSidebar","previous":{"title":"Introduction to Physical AI","permalink":"/physical-ai-and-humanoid-robotics/docs/module1/week1-intro-physical-ai"},"next":{"title":"ROS 2 Architecture","permalink":"/physical-ai-and-humanoid-robotics/docs/module1/week3-ros-fundamentals"}}');var i=r(4848),t=r(8453);const o={sidebar_position:2,title:"Sensors and Embodiment",description:"Understanding robot sensor systems - LiDAR, cameras, IMUs, and force sensors for Physical AI",keywords:["sensors","lidar","imu","camera","force-sensor","embodiment"]},a="Week 2: Sensors and Embodiment",l={},c=[{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"The Physics (Why)",id:"the-physics-why",level:2},{value:"The Analogy (Mental Model)",id:"the-analogy-mental-model",level:2},{value:"The Visualization (Sensor Data Flow)",id:"the-visualization-sensor-data-flow",level:2},{value:"The Code (Implementation)",id:"the-code-implementation",level:2},{value:"LiDAR Processing",id:"lidar-processing",level:3},{value:"IMU Processing",id:"imu-processing",level:3},{value:"Force/Torque Sensing",id:"forcetorque-sensing",level:3},{value:"The Hardware Reality (Warning)",id:"the-hardware-reality-warning",level:2},{value:"Sensor Selection Guide",id:"sensor-selection-guide",level:3},{value:"Common Sensor Issues",id:"common-sensor-issues",level:3},{value:"Assessment",id:"assessment",level:2},{value:"Recall",id:"recall",level:3},{value:"Apply",id:"apply",level:3},{value:"Analyze",id:"analyze",level:3}];function d(e){const n={admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"week-2-sensors-and-embodiment",children:"Week 2: Sensors and Embodiment"})}),"\n",(0,i.jsx)(n.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,i.jsx)(n.p,{children:"By the end of this chapter, you should be able to:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Explain the role of different sensor types in robotic perception"}),"\n",(0,i.jsx)(n.li,{children:"Understand how LiDAR, cameras, IMUs, and force sensors work"}),"\n",(0,i.jsx)(n.li,{children:"Design a sensor suite for a humanoid robot application"}),"\n",(0,i.jsx)(n.li,{children:"Implement basic sensor data processing in Python"}),"\n",(0,i.jsx)(n.li,{children:"Recognize sensor limitations and failure modes"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"the-physics-why",children:"The Physics (Why)"}),"\n",(0,i.jsxs)(n.p,{children:["For a robot to interact with the physical world, it must first ",(0,i.jsx)(n.strong,{children:"perceive"})," that world. Sensors are the robot's interface to reality\u2014they convert physical phenomena (light, distance, acceleration, force) into electrical signals that can be processed by computers."]}),"\n",(0,i.jsx)(n.p,{children:"The choice of sensors fundamentally shapes what a robot can do:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Without vision"}),", a robot cannot recognize objects or navigate visually"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Without proprioception"})," (joint sensors), a robot cannot know its own pose"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Without force sensing"}),", a robot cannot safely interact with fragile objects"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Without an IMU"}),", a humanoid cannot maintain balance"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Embodiment means the robot's intelligence is inseparable from its physical form and sensors."}),"\n",(0,i.jsx)(n.h2,{id:"the-analogy-mental-model",children:"The Analogy (Mental Model)"}),"\n",(0,i.jsx)(n.p,{children:"Think of robot sensors like human senses, but with different strengths and weaknesses:"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Human Sense"}),(0,i.jsx)(n.th,{children:"Robot Equivalent"}),(0,i.jsx)(n.th,{children:"Strengths"}),(0,i.jsx)(n.th,{children:"Weaknesses"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Eyes"})}),(0,i.jsx)(n.td,{children:"Cameras"}),(0,i.jsx)(n.td,{children:"Rich detail, color"}),(0,i.jsx)(n.td,{children:"Lighting dependent"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Echolocation"})}),(0,i.jsx)(n.td,{children:"LiDAR/Sonar"}),(0,i.jsx)(n.td,{children:"Precise distance"}),(0,i.jsx)(n.td,{children:"No color/texture"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Inner Ear"})}),(0,i.jsx)(n.td,{children:"IMU"}),(0,i.jsx)(n.td,{children:"Fast response"}),(0,i.jsx)(n.td,{children:"Drift over time"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Touch"})}),(0,i.jsx)(n.td,{children:"Force/Torque Sensors"}),(0,i.jsx)(n.td,{children:"Direct contact info"}),(0,i.jsx)(n.td,{children:"Limited coverage"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Proprioception"})}),(0,i.jsx)(n.td,{children:"Joint Encoders"}),(0,i.jsx)(n.td,{children:"Precise joint angles"}),(0,i.jsx)(n.td,{children:"No external info"})]})]})]}),"\n",(0,i.jsxs)(n.p,{children:["Just as humans fuse multiple senses to understand the world, robots use ",(0,i.jsx)(n.strong,{children:"sensor fusion"})," to combine data from multiple sources."]}),"\n",(0,i.jsx)(n.h2,{id:"the-visualization-sensor-data-flow",children:"The Visualization (Sensor Data Flow)"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-mermaid",children:'graph TB\r\n    subgraph "Exteroceptive Sensors"\r\n        A[RGB Camera] --\x3e|"640x480 @ 30Hz"| E[Image Processing]\r\n        B[Depth Camera] --\x3e|"Point Cloud"| F[3D Reconstruction]\r\n        C[LiDAR] --\x3e|"360\xb0 Scan"| F\r\n    end\r\n    \r\n    subgraph "Proprioceptive Sensors"\r\n        D[IMU] --\x3e|"Accel + Gyro"| G[State Estimation]\r\n        H[Joint Encoders] --\x3e|"Angles"| G\r\n        I[Force/Torque] --\x3e|"Contact Forces"| G\r\n    end\r\n    \r\n    subgraph "Sensor Fusion"\r\n        E --\x3e J[World Model]\r\n        F --\x3e J\r\n        G --\x3e J\r\n    end\r\n    \r\n    J --\x3e K[Planning & Control]\n'})}),"\n",(0,i.jsx)(n.h2,{id:"the-code-implementation",children:"The Code (Implementation)"}),"\n",(0,i.jsx)(n.h3,{id:"lidar-processing",children:"LiDAR Processing"}),"\n",(0,i.jsx)(n.p,{children:"LiDAR (Light Detection and Ranging) measures distances using laser pulses:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n"""\r\nLiDAR data processing for obstacle detection.\r\nDemonstrates basic point cloud processing.\r\n"""\r\n\r\nimport numpy as np\r\nfrom dataclasses import dataclass\r\nfrom typing import List, Tuple\r\n\r\n\r\n@dataclass\r\nclass LiDARScan:\r\n    """Represents a single LiDAR scan."""\r\n    angles: np.ndarray      # Angles in radians\r\n    distances: np.ndarray   # Distances in meters\r\n    intensities: np.ndarray # Return intensities\r\n    timestamp: float        # Scan timestamp\r\n\r\n\r\ndef polar_to_cartesian(scan: LiDARScan) -> np.ndarray:\r\n    """\r\n    Convert polar LiDAR data to Cartesian coordinates.\r\n    \r\n    Args:\r\n        scan: LiDAR scan in polar coordinates\r\n        \r\n    Returns:\r\n        Nx2 array of (x, y) points in meters\r\n    """\r\n    x = scan.distances * np.cos(scan.angles)\r\n    y = scan.distances * np.sin(scan.angles)\r\n    return np.column_stack([x, y])\r\n\r\n\r\ndef detect_obstacles(\r\n    points: np.ndarray,\r\n    robot_radius: float = 0.3,\r\n    safety_margin: float = 0.5\r\n) -> List[Tuple[float, float, float]]:\r\n    """\r\n    Detect obstacles within safety distance.\r\n    \r\n    Args:\r\n        points: Nx2 array of (x, y) points\r\n        robot_radius: Robot\'s physical radius in meters\r\n        safety_margin: Additional safety buffer in meters\r\n        \r\n    Returns:\r\n        List of (x, y, distance) for nearby obstacles\r\n    """\r\n    obstacles = []\r\n    min_safe_distance = robot_radius + safety_margin\r\n    \r\n    for point in points:\r\n        distance = np.linalg.norm(point)\r\n        if distance < min_safe_distance and distance > 0.1:  # Filter noise\r\n            obstacles.append((point[0], point[1], distance))\r\n    \r\n    return obstacles\r\n\r\n\r\n# Example usage\r\nif __name__ == "__main__":\r\n    # Simulate a LiDAR scan\r\n    angles = np.linspace(0, 2 * np.pi, 360)\r\n    distances = np.random.uniform(0.5, 10.0, 360)\r\n    distances[45:55] = 0.4  # Simulate nearby obstacle\r\n    \r\n    scan = LiDARScan(\r\n        angles=angles,\r\n        distances=distances,\r\n        intensities=np.ones(360),\r\n        timestamp=0.0\r\n    )\r\n    \r\n    points = polar_to_cartesian(scan)\r\n    obstacles = detect_obstacles(points)\r\n    \r\n    print(f"Detected {len(obstacles)} obstacles within safety zone")\n'})}),"\n",(0,i.jsx)(n.h3,{id:"imu-processing",children:"IMU Processing"}),"\n",(0,i.jsx)(n.p,{children:"The IMU (Inertial Measurement Unit) provides acceleration and angular velocity:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n"""\r\nIMU data processing for orientation estimation.\r\nImplements a simple complementary filter.\r\n"""\r\n\r\nimport numpy as np\r\nfrom dataclasses import dataclass\r\nimport math\r\n\r\n\r\n@dataclass\r\nclass IMUReading:\r\n    """Single IMU measurement."""\r\n    accel_x: float  # m/s\xb2\r\n    accel_y: float\r\n    accel_z: float\r\n    gyro_x: float   # rad/s\r\n    gyro_y: float\r\n    gyro_z: float\r\n    timestamp: float\r\n\r\n\r\nclass ComplementaryFilter:\r\n    """\r\n    Fuses accelerometer and gyroscope data for orientation.\r\n    \r\n    The accelerometer provides absolute orientation (from gravity)\r\n    but is noisy. The gyroscope provides smooth rotation rates\r\n    but drifts over time. The complementary filter combines both.\r\n    """\r\n    \r\n    def __init__(self, alpha: float = 0.98):\r\n        """\r\n        Args:\r\n            alpha: Filter coefficient (0.98 = trust gyro 98%)\r\n        """\r\n        self.alpha = alpha\r\n        self.roll = 0.0   # Rotation around X\r\n        self.pitch = 0.0  # Rotation around Y\r\n        self.last_time = None\r\n    \r\n    def update(self, imu: IMUReading) -> tuple[float, float]:\r\n        """\r\n        Update orientation estimate with new IMU reading.\r\n        \r\n        Args:\r\n            imu: New IMU measurement\r\n            \r\n        Returns:\r\n            (roll, pitch) in radians\r\n        """\r\n        # Calculate dt\r\n        if self.last_time is None:\r\n            self.last_time = imu.timestamp\r\n            return (0.0, 0.0)\r\n        \r\n        dt = imu.timestamp - self.last_time\r\n        self.last_time = imu.timestamp\r\n        \r\n        # Orientation from accelerometer (absolute but noisy)\r\n        accel_roll = math.atan2(imu.accel_y, imu.accel_z)\r\n        accel_pitch = math.atan2(\r\n            -imu.accel_x,\r\n            math.sqrt(imu.accel_y**2 + imu.accel_z**2)\r\n        )\r\n        \r\n        # Orientation from gyroscope integration (smooth but drifts)\r\n        gyro_roll = self.roll + imu.gyro_x * dt\r\n        gyro_pitch = self.pitch + imu.gyro_y * dt\r\n        \r\n        # Complementary filter: combine both estimates\r\n        self.roll = self.alpha * gyro_roll + (1 - self.alpha) * accel_roll\r\n        self.pitch = self.alpha * gyro_pitch + (1 - self.alpha) * accel_pitch\r\n        \r\n        return (self.roll, self.pitch)\r\n    \r\n    def is_falling(self, threshold: float = 0.5) -> bool:\r\n        """\r\n        Check if robot is tilting dangerously.\r\n        \r\n        Args:\r\n            threshold: Maximum safe tilt in radians (~28 degrees)\r\n            \r\n        Returns:\r\n            True if robot may be falling\r\n        """\r\n        return abs(self.roll) > threshold or abs(self.pitch) > threshold\n'})}),"\n",(0,i.jsx)(n.h3,{id:"forcetorque-sensing",children:"Force/Torque Sensing"}),"\n",(0,i.jsx)(n.p,{children:"Force sensors enable safe physical interaction:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n"""\r\nForce/Torque sensor processing for safe manipulation.\r\n"""\r\n\r\nimport numpy as np\r\nfrom dataclasses import dataclass\r\nfrom typing import Optional\r\n\r\n\r\n@dataclass\r\nclass ForceTorque:\r\n    """6-axis force/torque measurement."""\r\n    fx: float  # Force X (N)\r\n    fy: float  # Force Y (N)\r\n    fz: float  # Force Z (N)\r\n    tx: float  # Torque X (Nm)\r\n    ty: float  # Torque Y (Nm)\r\n    tz: float  # Torque Z (Nm)\r\n    \r\n    @property\r\n    def force_magnitude(self) -> float:\r\n        """Total force magnitude."""\r\n        return np.sqrt(self.fx**2 + self.fy**2 + self.fz**2)\r\n    \r\n    @property\r\n    def torque_magnitude(self) -> float:\r\n        """Total torque magnitude."""\r\n        return np.sqrt(self.tx**2 + self.ty**2 + self.tz**2)\r\n\r\n\r\nclass SafetyMonitor:\r\n    """\r\n    Monitors force/torque for safety limits.\r\n    Essential for human-robot interaction.\r\n    """\r\n    \r\n    def __init__(\r\n        self,\r\n        max_force: float = 50.0,    # Newtons\r\n        max_torque: float = 10.0,   # Newton-meters\r\n    ):\r\n        self.max_force = max_force\r\n        self.max_torque = max_torque\r\n        self.violation_count = 0\r\n    \r\n    def check(self, ft: ForceTorque) -> Optional[str]:\r\n        """\r\n        Check if force/torque exceeds safety limits.\r\n        \r\n        Returns:\r\n            None if safe, error message if limit exceeded\r\n        """\r\n        if ft.force_magnitude > self.max_force:\r\n            self.violation_count += 1\r\n            return f"FORCE LIMIT: {ft.force_magnitude:.1f}N > {self.max_force}N"\r\n        \r\n        if ft.torque_magnitude > self.max_torque:\r\n            self.violation_count += 1\r\n            return f"TORQUE LIMIT: {ft.torque_magnitude:.2f}Nm > {self.max_torque}Nm"\r\n        \r\n        return None\r\n    \r\n    def should_emergency_stop(self, consecutive_violations: int = 3) -> bool:\r\n        """Check if emergency stop should be triggered."""\r\n        return self.violation_count >= consecutive_violations\n'})}),"\n",(0,i.jsx)(n.h2,{id:"the-hardware-reality-warning",children:"The Hardware Reality (Warning)"}),"\n",(0,i.jsxs)(n.admonition,{title:"Sensor Failure Modes",type:"danger",children:[(0,i.jsx)(n.p,{children:"Sensors can fail in ways that are not immediately obvious:"}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"LiDAR"}),": Reflective surfaces cause false readings"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Cameras"}),": Overexposure in bright light, noise in darkness"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"IMU"}),": Drift accumulates over time (can be degrees per minute)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Force sensors"}),": Temperature changes affect calibration"]}),"\n"]})]}),"\n",(0,i.jsx)(n.h3,{id:"sensor-selection-guide",children:"Sensor Selection Guide"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Sensor"}),(0,i.jsx)(n.th,{children:"Best For"}),(0,i.jsx)(n.th,{children:"Limitations"}),(0,i.jsx)(n.th,{children:"Typical Cost"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Intel RealSense D435i"})}),(0,i.jsx)(n.td,{children:"Indoor SLAM, manipulation"}),(0,i.jsx)(n.td,{children:"Poor outdoors"}),(0,i.jsx)(n.td,{children:"$350"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Velodyne VLP-16"})}),(0,i.jsx)(n.td,{children:"Outdoor navigation"}),(0,i.jsx)(n.td,{children:"Expensive, bulky"}),(0,i.jsx)(n.td,{children:"$4,000"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"BNO055 IMU"})}),(0,i.jsx)(n.td,{children:"Orientation, motion"}),(0,i.jsx)(n.td,{children:"Drift, vibration sensitive"}),(0,i.jsx)(n.td,{children:"$30"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"ATI Mini45"})}),(0,i.jsx)(n.td,{children:"Precision force control"}),(0,i.jsx)(n.td,{children:"Fragile, expensive"}),(0,i.jsx)(n.td,{children:"$5,000"})]})]})]}),"\n",(0,i.jsx)(n.admonition,{title:"Sensor Fusion is Essential",type:"tip",children:(0,i.jsx)(n.p,{children:"No single sensor is reliable enough for safety-critical applications. Always use multiple sensors and cross-validate their readings."})}),"\n",(0,i.jsx)(n.h3,{id:"common-sensor-issues",children:"Common Sensor Issues"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-mermaid",children:"graph TD\r\n    A[Sensor Reading] --\x3e B{Sanity Check}\r\n    B --\x3e|Pass| C[Use Data]\r\n    B --\x3e|Fail| D{Redundant Sensor?}\r\n    D --\x3e|Yes| E[Use Backup]\r\n    D --\x3e|No| F[Safe State]\r\n    \r\n    style F fill:#ffcdd2\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Sanity checks to implement:"})}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Range check"}),": Is the value physically possible?"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Rate check"}),": Did the value change too fast?"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Consistency check"}),": Do multiple sensors agree?"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Timestamp check"}),": Is the data fresh?"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"assessment",children:"Assessment"}),"\n",(0,i.jsx)(n.h3,{id:"recall",children:"Recall"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"What is the difference between exteroceptive and proprioceptive sensors?"}),"\n",(0,i.jsx)(n.li,{children:"Why does an IMU drift over time, and how can this be mitigated?"}),"\n",(0,i.jsx)(n.li,{children:"What physical phenomenon does LiDAR use to measure distance?"}),"\n",(0,i.jsx)(n.li,{children:"Why is force sensing important for human-robot interaction?"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"apply",children:"Apply"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Design a sensor suite for a humanoid robot that needs to:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Navigate through a cluttered warehouse"}),"\n",(0,i.jsx)(n.li,{children:"Pick up boxes of varying weights"}),"\n",(0,i.jsx)(n.li,{children:"Avoid collisions with humans"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Justify each sensor choice."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Implement a simple sensor fusion algorithm that combines camera-based object detection with LiDAR distance measurements to estimate object positions in 3D."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Write a safety monitor that triggers an emergency stop if:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"IMU detects tilt > 30 degrees"}),"\n",(0,i.jsx)(n.li,{children:"Force sensor reads > 100N"}),"\n",(0,i.jsx)(n.li,{children:"LiDAR detects obstacle < 0.2m"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"analyze",children:"Analyze"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"A robot's camera-based navigation works well in the lab but fails in a factory with reflective floors. Diagnose the problem and propose solutions."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Compare the trade-offs between using a single high-quality sensor versus multiple cheaper sensors with fusion. When would you choose each approach?"}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Why might a humanoid robot need different sensor configurations for walking versus manipulation tasks?"}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453(e,n,r){r.d(n,{R:()=>o,x:()=>a});var s=r(6540);const i={},t=s.createContext(i);function o(e){const n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);