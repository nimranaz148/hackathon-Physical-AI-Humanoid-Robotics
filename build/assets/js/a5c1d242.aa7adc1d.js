"use strict";(globalThis.webpackChunkphysical_ai_textbook=globalThis.webpackChunkphysical_ai_textbook||[]).push([[225],{6375(e,n,r){r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>t,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"module2/week7-gazebo-unity","title":"Unity Integration and Sensor Simulation","description":"High-fidelity rendering with Unity and simulating LiDAR, depth cameras, and IMUs","source":"@site/docs/module2/week7-gazebo-unity.md","sourceDirName":"module2","slug":"/module2/week7-gazebo-unity","permalink":"/physical-ai-and-humanoid-robotics/docs/module2/week7-gazebo-unity","draft":false,"unlisted":false,"editUrl":"https://github.com/nimranaz148/docs/module2/week7-gazebo-unity.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"title":"Unity Integration and Sensor Simulation","description":"High-fidelity rendering with Unity and simulating LiDAR, depth cameras, and IMUs","keywords":["unity","sensors","lidar","depth-camera","imu","simulation"]},"sidebar":"tutorialSidebar","previous":{"title":"Robot Simulation with Gazebo","permalink":"/physical-ai-and-humanoid-robotics/docs/module2/week6-gazebo"},"next":{"title":"Module 3: NVIDIA Isaac","permalink":"/physical-ai-and-humanoid-robotics/docs/category/module-3-nvidia-isaac"}}');var s=r(4848),a=r(8453);const t={sidebar_position:2,title:"Unity Integration and Sensor Simulation",description:"High-fidelity rendering with Unity and simulating LiDAR, depth cameras, and IMUs",keywords:["unity","sensors","lidar","depth-camera","imu","simulation"]},o="Week 7: Unity Integration and Sensor Simulation",l={},d=[{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"The Physics (Why)",id:"the-physics-why",level:2},{value:"The Analogy (Mental Model)",id:"the-analogy-mental-model",level:2},{value:"The Visualization (Sensor Pipeline)",id:"the-visualization-sensor-pipeline",level:2},{value:"The Code (Implementation)",id:"the-code-implementation",level:2},{value:"LiDAR Sensor Configuration (SDF)",id:"lidar-sensor-configuration-sdf",level:3},{value:"Depth Camera Configuration",id:"depth-camera-configuration",level:3},{value:"IMU Configuration",id:"imu-configuration",level:3},{value:"Processing Sensor Data in ROS 2",id:"processing-sensor-data-in-ros-2",level:3},{value:"The Hardware Reality (Warning)",id:"the-hardware-reality-warning",level:2},{value:"Assessment",id:"assessment",level:2},{value:"Recall",id:"recall",level:3},{value:"Apply",id:"apply",level:3},{value:"Analyze",id:"analyze",level:3}];function c(e){const n={admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"week-7-unity-integration-and-sensor-simulation",children:"Week 7: Unity Integration and Sensor Simulation"})}),"\n",(0,s.jsx)(n.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,s.jsx)(n.p,{children:"By the end of this chapter, you should be able to:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Understand when to use Unity vs. Gazebo for robot simulation"}),"\n",(0,s.jsx)(n.li,{children:"Configure LiDAR, depth camera, and IMU sensors in simulation"}),"\n",(0,s.jsx)(n.li,{children:"Process simulated sensor data in ROS 2"}),"\n",(0,s.jsx)(n.li,{children:"Create realistic sensor noise models"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"the-physics-why",children:"The Physics (Why)"}),"\n",(0,s.jsxs)(n.p,{children:["While Gazebo excels at physics simulation, ",(0,s.jsx)(n.strong,{children:"Unity"})," provides superior visual fidelity\u2014essential for training computer vision models. Additionally, accurate ",(0,s.jsx)(n.strong,{children:"sensor simulation"})," is critical because robots perceive the world through sensors, not direct access to simulation state."]}),"\n",(0,s.jsx)(n.h2,{id:"the-analogy-mental-model",children:"The Analogy (Mental Model)"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Simulator"}),(0,s.jsx)(n.th,{children:"Strength"}),(0,s.jsx)(n.th,{children:"Best For"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Gazebo"})}),(0,s.jsx)(n.td,{children:"Physics accuracy"}),(0,s.jsx)(n.td,{children:"Control algorithms, dynamics"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Unity"})}),(0,s.jsx)(n.td,{children:"Visual realism"}),(0,s.jsx)(n.td,{children:"Vision AI, human interaction"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Isaac Sim"})}),(0,s.jsx)(n.td,{children:"Both + GPU"}),(0,s.jsx)(n.td,{children:"Production sim-to-real"})]})]})]}),"\n",(0,s.jsx)(n.p,{children:"Think of it like movie production: Gazebo is the stunt coordinator (physics), Unity is the cinematographer (visuals)."}),"\n",(0,s.jsx)(n.h2,{id:"the-visualization-sensor-pipeline",children:"The Visualization (Sensor Pipeline)"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-mermaid",children:'graph LR\r\n    subgraph "Physical World"\r\n        A[Robot] --\x3e B[Environment]\r\n    end\r\n    \r\n    subgraph "Sensor Simulation"\r\n        B --\x3e C[LiDAR Plugin]\r\n        B --\x3e D[Camera Plugin]\r\n        B --\x3e E[IMU Plugin]\r\n    end\r\n    \r\n    subgraph "ROS 2 Topics"\r\n        C --\x3e F[/scan]\r\n        D --\x3e G[/camera/image]\r\n        D --\x3e H[/camera/depth]\r\n        E --\x3e I[/imu]\r\n    end\r\n    \r\n    subgraph "Perception Stack"\r\n        F --\x3e J[SLAM]\r\n        G --\x3e K[Object Detection]\r\n        H --\x3e J\r\n        I --\x3e L[State Estimation]\r\n    end\n'})}),"\n",(0,s.jsx)(n.h2,{id:"the-code-implementation",children:"The Code (Implementation)"}),"\n",(0,s.jsx)(n.h3,{id:"lidar-sensor-configuration-sdf",children:"LiDAR Sensor Configuration (SDF)"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-xml",children:'\x3c!-- lidar_sensor.sdf - 2D LiDAR configuration --\x3e\r\n<sensor name="lidar" type="gpu_lidar">\r\n  <pose>0 0 0.1 0 0 0</pose>\r\n  <topic>/scan</topic>\r\n  <update_rate>10</update_rate>\r\n  <lidar>\r\n    <scan>\r\n      <horizontal>\r\n        <samples>360</samples>\r\n        <resolution>1</resolution>\r\n        <min_angle>-3.14159</min_angle>\r\n        <max_angle>3.14159</max_angle>\r\n      </horizontal>\r\n    </scan>\r\n    <range>\r\n      <min>0.1</min>\r\n      <max>10.0</max>\r\n      <resolution>0.01</resolution>\r\n    </range>\r\n    <noise>\r\n      <type>gaussian</type>\r\n      <mean>0.0</mean>\r\n      <stddev>0.01</stddev>\r\n    </noise>\r\n  </lidar>\r\n</sensor>\n'})}),"\n",(0,s.jsx)(n.h3,{id:"depth-camera-configuration",children:"Depth Camera Configuration"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-xml",children:'\x3c!-- depth_camera.sdf - Intel RealSense D435i style --\x3e\r\n<sensor name="depth_camera" type="depth_camera">\r\n  <pose>0.1 0 0.5 0 0 0</pose>\r\n  <update_rate>30</update_rate>\r\n  <camera>\r\n    <horizontal_fov>1.047</horizontal_fov>\r\n    <image>\r\n      <width>640</width>\r\n      <height>480</height>\r\n      <format>R_FLOAT32</format>\r\n    </image>\r\n    <clip>\r\n      <near>0.1</near>\r\n      <far>10.0</far>\r\n    </clip>\r\n    <noise>\r\n      <type>gaussian</type>\r\n      <mean>0.0</mean>\r\n      <stddev>0.005</stddev>\r\n    </noise>\r\n  </camera>\r\n</sensor>\n'})}),"\n",(0,s.jsx)(n.h3,{id:"imu-configuration",children:"IMU Configuration"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-xml",children:'\x3c!-- imu_sensor.sdf - 6-axis IMU --\x3e\r\n<sensor name="imu" type="imu">\r\n  <pose>0 0 0.3 0 0 0</pose>\r\n  <update_rate>100</update_rate>\r\n  <imu>\r\n    <angular_velocity>\r\n      <x><noise type="gaussian"><mean>0</mean><stddev>0.001</stddev></noise></x>\r\n      <y><noise type="gaussian"><mean>0</mean><stddev>0.001</stddev></noise></y>\r\n      <z><noise type="gaussian"><mean>0</mean><stddev>0.001</stddev></noise></z>\r\n    </angular_velocity>\r\n    <linear_acceleration>\r\n      <x><noise type="gaussian"><mean>0</mean><stddev>0.01</stddev></noise></x>\r\n      <y><noise type="gaussian"><mean>0</mean><stddev>0.01</stddev></noise></y>\r\n      <z><noise type="gaussian"><mean>0</mean><stddev>0.01</stddev></noise></z>\r\n    </linear_acceleration>\r\n  </imu>\r\n</sensor>\n'})}),"\n",(0,s.jsx)(n.h3,{id:"processing-sensor-data-in-ros-2",children:"Processing Sensor Data in ROS 2"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n"""\r\nsensor_processor.py - Process simulated sensor data.\r\n"""\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import LaserScan, Image, Imu\r\nfrom cv_bridge import CvBridge\r\nimport numpy as np\r\n\r\n\r\nclass SensorProcessor(Node):\r\n    """Processes data from simulated sensors."""\r\n    \r\n    def __init__(self):\r\n        super().__init__(\'sensor_processor\')\r\n        \r\n        self.bridge = CvBridge()\r\n        \r\n        # LiDAR subscriber\r\n        self.lidar_sub = self.create_subscription(\r\n            LaserScan, \'/scan\', self.lidar_callback, 10\r\n        )\r\n        \r\n        # Depth camera subscriber\r\n        self.depth_sub = self.create_subscription(\r\n            Image, \'/camera/depth\', self.depth_callback, 10\r\n        )\r\n        \r\n        # IMU subscriber\r\n        self.imu_sub = self.create_subscription(\r\n            Imu, \'/imu\', self.imu_callback, 10\r\n        )\r\n        \r\n        self.get_logger().info(\'Sensor processor started\')\r\n    \r\n    def lidar_callback(self, msg: LaserScan):\r\n        """Process LiDAR scan for obstacle detection."""\r\n        ranges = np.array(msg.ranges)\r\n        \r\n        # Find minimum distance (closest obstacle)\r\n        valid_ranges = ranges[np.isfinite(ranges)]\r\n        if len(valid_ranges) > 0:\r\n            min_dist = np.min(valid_ranges)\r\n            min_idx = np.argmin(valid_ranges)\r\n            angle = msg.angle_min + min_idx * msg.angle_increment\r\n            \r\n            if min_dist < 0.5:\r\n                self.get_logger().warn(\r\n                    f\'Obstacle at {min_dist:.2f}m, angle {np.degrees(angle):.1f}\xb0\'\r\n                )\r\n    \r\n    def depth_callback(self, msg: Image):\r\n        """Process depth image for 3D perception."""\r\n        depth_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding=\'32FC1\')\r\n        \r\n        # Calculate average depth in center region\r\n        h, w = depth_image.shape\r\n        center = depth_image[h//3:2*h//3, w//3:2*w//3]\r\n        avg_depth = np.nanmean(center)\r\n        \r\n        self.get_logger().debug(f\'Center depth: {avg_depth:.2f}m\')\r\n    \r\n    def imu_callback(self, msg: Imu):\r\n        """Process IMU for orientation estimation."""\r\n        # Extract orientation (quaternion)\r\n        q = msg.orientation\r\n        \r\n        # Extract angular velocity\r\n        omega = msg.angular_velocity\r\n        \r\n        # Extract linear acceleration\r\n        accel = msg.linear_acceleration\r\n        \r\n        # Simple tilt detection from accelerometer\r\n        tilt_x = np.arctan2(accel.y, accel.z)\r\n        tilt_y = np.arctan2(-accel.x, np.sqrt(accel.y**2 + accel.z**2))\r\n        \r\n        if abs(tilt_x) > 0.3 or abs(tilt_y) > 0.3:\r\n            self.get_logger().warn(\r\n                f\'High tilt detected: roll={np.degrees(tilt_x):.1f}\xb0, \'\r\n                f\'pitch={np.degrees(tilt_y):.1f}\xb0\'\r\n            )\r\n\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = SensorProcessor()\r\n    rclpy.spin(node)\r\n    node.destroy_node()\r\n    rclpy.shutdown()\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"the-hardware-reality-warning",children:"The Hardware Reality (Warning)"}),"\n",(0,s.jsxs)(n.admonition,{title:"Sensor Noise is Critical",type:"danger",children:[(0,s.jsx)(n.p,{children:"Simulated sensors without noise produce unrealistically clean data. Always add realistic noise models:"}),(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"LiDAR"}),": Gaussian noise + occasional dropouts"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Camera"}),": Motion blur, exposure variation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"IMU"}),": Bias drift, temperature effects"]}),"\n"]})]}),"\n",(0,s.jsx)(n.admonition,{title:"Unity for Vision AI",type:"tip",children:(0,s.jsx)(n.p,{children:"If training neural networks for object detection or segmentation, Unity's photorealistic rendering produces better training data than Gazebo's basic graphics."})}),"\n",(0,s.jsx)(n.h2,{id:"assessment",children:"Assessment"}),"\n",(0,s.jsx)(n.h3,{id:"recall",children:"Recall"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"What are the key differences between Gazebo and Unity for robotics?"}),"\n",(0,s.jsx)(n.li,{children:"What noise parameters should be configured for a simulated IMU?"}),"\n",(0,s.jsx)(n.li,{children:"How do you process depth images in ROS 2?"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"apply",children:"Apply"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Configure a 3D LiDAR sensor with 16 vertical beams and realistic noise."}),"\n",(0,s.jsx)(n.li,{children:"Write a node that fuses LiDAR and depth camera data for obstacle detection."}),"\n",(0,s.jsx)(n.li,{children:"Create a sensor noise model that varies based on distance (more noise at longer ranges)."}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"analyze",children:"Analyze"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Why might a vision algorithm trained on Unity data fail on real camera images?"}),"\n",(0,s.jsx)(n.li,{children:"Compare the computational cost of simulating LiDAR vs. depth cameras."}),"\n",(0,s.jsx)(n.li,{children:"Design a sensor suite for a humanoid robot that must navigate outdoors."}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},8453(e,n,r){r.d(n,{R:()=>t,x:()=>o});var i=r(6540);const s={},a=i.createContext(s);function t(e){const n=i.useContext(a);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:t(e.components),i.createElement(a.Provider,{value:n},e.children)}}}]);