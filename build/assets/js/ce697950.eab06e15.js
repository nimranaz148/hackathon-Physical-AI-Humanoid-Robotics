"use strict";(globalThis.webpackChunkphysical_ai_textbook=globalThis.webpackChunkphysical_ai_textbook||[]).push([[75],{3665(n,e,r){r.r(e),r.d(e,{assets:()=>l,contentTitle:()=>s,default:()=>m,frontMatter:()=>t,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"module3/week10-isaac-3","title":"Sim-to-Real Transfer","description":"Bridging the reality gap - domain randomization, reinforcement learning, and deploying simulation-trained models to real robots","source":"@site/docs/module3/week10-isaac-3.md","sourceDirName":"module3","slug":"/module3/week10-isaac-3","permalink":"/physical-ai-and-humanoid-robotics/docs/module3/week10-isaac-3","draft":false,"unlisted":false,"editUrl":"https://github.com/nimranaz148/docs/module3/week10-isaac-3.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3,"title":"Sim-to-Real Transfer","description":"Bridging the reality gap - domain randomization, reinforcement learning, and deploying simulation-trained models to real robots","keywords":["sim-to-real","domain-randomization","reinforcement-learning","transfer-learning"]},"sidebar":"tutorialSidebar","previous":{"title":"Isaac ROS and VSLAM","permalink":"/physical-ai-and-humanoid-robotics/docs/module3/week9-isaac-2"},"next":{"title":"Module 4: Vision-Language-Action","permalink":"/physical-ai-and-humanoid-robotics/docs/category/module-4-vision-language-action"}}');var a=r(4848),o=r(8453);const t={sidebar_position:3,title:"Sim-to-Real Transfer",description:"Bridging the reality gap - domain randomization, reinforcement learning, and deploying simulation-trained models to real robots",keywords:["sim-to-real","domain-randomization","reinforcement-learning","transfer-learning"]},s="Sim-to-Real Transfer",l={},d=[{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"The Physics (Why)",id:"the-physics-why",level:2},{value:"The Analogy (Mental Model)",id:"the-analogy-mental-model",level:2},{value:"The Visualization (Sim-to-Real Pipeline)",id:"the-visualization-sim-to-real-pipeline",level:2},{value:"The Code (Implementation)",id:"the-code-implementation",level:2},{value:"Domain Randomization",id:"domain-randomization",level:3},{value:"Reinforcement Learning Training",id:"reinforcement-learning-training",level:3},{value:"Deployment to Real Robot",id:"deployment-to-real-robot",level:3},{value:"The Hardware Reality (Warning)",id:"the-hardware-reality-warning",level:2},{value:"Debugging Sim-to-Real Gaps",id:"debugging-sim-to-real-gaps",level:3},{value:"Assessment",id:"assessment",level:2},{value:"Recall",id:"recall",level:3},{value:"Apply",id:"apply",level:3},{value:"Analyze",id:"analyze",level:3}];function c(n){const e={admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,o.R)(),...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.header,{children:(0,a.jsx)(e.h1,{id:"sim-to-real-transfer",children:"Sim-to-Real Transfer"})}),"\n",(0,a.jsx)(e.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,a.jsx)(e.p,{children:"By the end of this chapter, you should be able to:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Understand the reality gap and why sim-to-real transfer is challenging"}),"\n",(0,a.jsx)(e.li,{children:"Implement domain randomization to improve model robustness"}),"\n",(0,a.jsx)(e.li,{children:"Train reinforcement learning policies in simulation"}),"\n",(0,a.jsx)(e.li,{children:"Deploy simulation-trained models to real robots"}),"\n",(0,a.jsx)(e.li,{children:"Evaluate and debug sim-to-real transfer failures"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"the-physics-why",children:"The Physics (Why)"}),"\n",(0,a.jsxs)(e.p,{children:["The ",(0,a.jsx)(e.strong,{children:"reality gap"})," is the difference between simulated and real-world physics. Simulators make approximations:"]}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Friction"}),": Real friction is complex (static vs. dynamic, surface variations)"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Contact dynamics"}),": Real collisions involve deformation, vibration"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Actuator dynamics"}),": Real motors have delays, backlash, temperature effects"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Sensor noise"}),": Real sensors have noise patterns simulators can't perfectly model"]}),"\n"]}),"\n",(0,a.jsx)(e.p,{children:"A policy trained in simulation may fail catastrophically on a real robot because it learned to exploit simulator quirks rather than robust physical principles."}),"\n",(0,a.jsx)(e.h2,{id:"the-analogy-mental-model",children:"The Analogy (Mental Model)"}),"\n",(0,a.jsxs)(e.p,{children:["Think of sim-to-real transfer like ",(0,a.jsx)(e.strong,{children:"learning to drive in a video game, then driving a real car"}),":"]}),"\n",(0,a.jsxs)(e.table,{children:[(0,a.jsx)(e.thead,{children:(0,a.jsxs)(e.tr,{children:[(0,a.jsx)(e.th,{children:"Video Game"}),(0,a.jsx)(e.th,{children:"Real World"})]})}),(0,a.jsxs)(e.tbody,{children:[(0,a.jsxs)(e.tr,{children:[(0,a.jsx)(e.td,{children:"Perfect traction"}),(0,a.jsx)(e.td,{children:"Ice, gravel, wet roads"})]}),(0,a.jsxs)(e.tr,{children:[(0,a.jsx)(e.td,{children:"Instant response"}),(0,a.jsx)(e.td,{children:"Steering delay, brake fade"})]}),(0,a.jsxs)(e.tr,{children:[(0,a.jsx)(e.td,{children:"No consequences"}),(0,a.jsx)(e.td,{children:"Accidents cause damage"})]}),(0,a.jsxs)(e.tr,{children:[(0,a.jsx)(e.td,{children:"Reset on failure"}),(0,a.jsx)(e.td,{children:"No respawns"})]})]})]}),"\n",(0,a.jsx)(e.p,{children:"To succeed, you need to:"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Train with variety"}),": Different weather, road conditions"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Add noise"}),": Imperfect controls, sensor lag"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Be conservative"}),": Don't rely on perfect physics"]}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"the-visualization-sim-to-real-pipeline",children:"The Visualization (Sim-to-Real Pipeline)"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-mermaid",children:'graph TB\r\n    subgraph "Simulation Training"\r\n        A[Isaac Sim] --\x3e B[Domain Randomization]\r\n        B --\x3e C[RL Training]\r\n        C --\x3e D[Policy Network]\r\n    end\r\n    \r\n    subgraph "Transfer Techniques"\r\n        D --\x3e E[System Identification]\r\n        D --\x3e F[Fine-tuning]\r\n        D --\x3e G[Residual Learning]\r\n    end\r\n    \r\n    subgraph "Real Robot"\r\n        E --\x3e H[Calibrated Sim]\r\n        F --\x3e I[Real Data]\r\n        G --\x3e J[Hybrid Policy]\r\n        H --\x3e K[Deployment]\r\n        I --\x3e K\r\n        J --\x3e K\r\n    end\r\n    \r\n    K --\x3e L[Performance Evaluation]\r\n    L --\x3e|Gap detected| B\n'})}),"\n",(0,a.jsx)(e.h2,{id:"the-code-implementation",children:"The Code (Implementation)"}),"\n",(0,a.jsx)(e.h3,{id:"domain-randomization",children:"Domain Randomization"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n"""\r\ndomain_randomization.py - Randomize simulation parameters for robust training.\r\n"""\r\n\r\nimport numpy as np\r\nfrom dataclasses import dataclass\r\nfrom typing import Dict, Tuple\r\nimport random\r\n\r\n\r\n@dataclass\r\nclass RandomizationConfig:\r\n    """Configuration for domain randomization."""\r\n    # Physics parameters\r\n    friction_range: Tuple[float, float] = (0.5, 1.5)\r\n    mass_scale_range: Tuple[float, float] = (0.8, 1.2)\r\n    motor_strength_range: Tuple[float, float] = (0.9, 1.1)\r\n    \r\n    # Sensor noise\r\n    observation_noise_std: float = 0.01\r\n    action_delay_range: Tuple[int, int] = (0, 3)  # frames\r\n    \r\n    # Visual randomization\r\n    lighting_range: Tuple[float, float] = (0.5, 1.5)\r\n    texture_randomization: bool = True\r\n\r\n\r\nclass DomainRandomizer:\r\n    """Apply domain randomization to simulation."""\r\n    \r\n    def __init__(self, config: RandomizationConfig):\r\n        self.config = config\r\n        self.current_params = {}\r\n    \r\n    def randomize_physics(self) -> Dict[str, float]:\r\n        """Randomize physics parameters."""\r\n        params = {\r\n            \'friction\': np.random.uniform(*self.config.friction_range),\r\n            \'mass_scale\': np.random.uniform(*self.config.mass_scale_range),\r\n            \'motor_strength\': np.random.uniform(*self.config.motor_strength_range),\r\n        }\r\n        self.current_params.update(params)\r\n        return params\r\n    \r\n    def add_observation_noise(self, obs: np.ndarray) -> np.ndarray:\r\n        """Add noise to observations."""\r\n        noise = np.random.normal(0, self.config.observation_noise_std, obs.shape)\r\n        return obs + noise\r\n    \r\n    def apply_action_delay(self, action: np.ndarray, action_buffer: list) -> np.ndarray:\r\n        """Apply random action delay."""\r\n        delay = np.random.randint(*self.config.action_delay_range)\r\n        action_buffer.append(action)\r\n        \r\n        if len(action_buffer) > delay:\r\n            return action_buffer.pop(0)\r\n        return np.zeros_like(action)\r\n    \r\n    def randomize_visual(self) -> Dict[str, float]:\r\n        """Randomize visual parameters."""\r\n        params = {\r\n            \'lighting_intensity\': np.random.uniform(*self.config.lighting_range),\r\n            \'texture_id\': random.randint(0, 100) if self.config.texture_randomization else 0,\r\n        }\r\n        self.current_params.update(params)\r\n        return params\r\n\r\n\r\nclass RobustTrainingEnv:\r\n    """Training environment with domain randomization."""\r\n    \r\n    def __init__(self, base_env, randomizer: DomainRandomizer):\r\n        self.base_env = base_env\r\n        self.randomizer = randomizer\r\n        self.action_buffer = []\r\n        self.episode_count = 0\r\n    \r\n    def reset(self):\r\n        """Reset environment with new randomization."""\r\n        self.episode_count += 1\r\n        self.action_buffer = []\r\n        \r\n        # Randomize at start of each episode\r\n        physics_params = self.randomizer.randomize_physics()\r\n        visual_params = self.randomizer.randomize_visual()\r\n        \r\n        # Apply to simulation (implementation depends on simulator)\r\n        self.apply_randomization(physics_params, visual_params)\r\n        \r\n        obs = self.base_env.reset()\r\n        return self.randomizer.add_observation_noise(obs)\r\n    \r\n    def step(self, action):\r\n        """Step with randomization applied."""\r\n        # Apply action delay\r\n        delayed_action = self.randomizer.apply_action_delay(\r\n            action, self.action_buffer\r\n        )\r\n        \r\n        obs, reward, done, info = self.base_env.step(delayed_action)\r\n        \r\n        # Add observation noise\r\n        noisy_obs = self.randomizer.add_observation_noise(obs)\r\n        \r\n        return noisy_obs, reward, done, info\r\n    \r\n    def apply_randomization(self, physics_params, visual_params):\r\n        """Apply randomization to simulator."""\r\n        # Implementation depends on specific simulator\r\n        pass\n'})}),"\n",(0,a.jsx)(e.h3,{id:"reinforcement-learning-training",children:"Reinforcement Learning Training"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n"""\r\nrl_training.py - Train walking policy with PPO.\r\n"""\r\n\r\nimport numpy as np\r\nimport torch\r\nimport torch.nn as nn\r\nfrom typing import Tuple\r\n\r\n\r\nclass WalkingPolicy(nn.Module):\r\n    """Neural network policy for humanoid walking."""\r\n    \r\n    def __init__(self, obs_dim: int, action_dim: int, hidden_dim: int = 256):\r\n        super().__init__()\r\n        \r\n        # Actor network (policy)\r\n        self.actor = nn.Sequential(\r\n            nn.Linear(obs_dim, hidden_dim),\r\n            nn.ReLU(),\r\n            nn.Linear(hidden_dim, hidden_dim),\r\n            nn.ReLU(),\r\n            nn.Linear(hidden_dim, action_dim),\r\n            nn.Tanh()  # Actions in [-1, 1]\r\n        )\r\n        \r\n        # Critic network (value function)\r\n        self.critic = nn.Sequential(\r\n            nn.Linear(obs_dim, hidden_dim),\r\n            nn.ReLU(),\r\n            nn.Linear(hidden_dim, hidden_dim),\r\n            nn.ReLU(),\r\n            nn.Linear(hidden_dim, 1)\r\n        )\r\n        \r\n        # Log standard deviation for action distribution\r\n        self.log_std = nn.Parameter(torch.zeros(action_dim))\r\n    \r\n    def forward(self, obs: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\r\n        """Get action mean and value estimate."""\r\n        action_mean = self.actor(obs)\r\n        value = self.critic(obs)\r\n        return action_mean, value\r\n    \r\n    def get_action(self, obs: torch.Tensor, deterministic: bool = False):\r\n        """Sample action from policy."""\r\n        action_mean, _ = self.forward(obs)\r\n        \r\n        if deterministic:\r\n            return action_mean\r\n        \r\n        std = torch.exp(self.log_std)\r\n        dist = torch.distributions.Normal(action_mean, std)\r\n        action = dist.sample()\r\n        \r\n        return action, dist.log_prob(action).sum(-1)\r\n\r\n\r\nclass WalkingReward:\r\n    """Reward function for humanoid walking."""\r\n    \r\n    def __init__(self):\r\n        # Reward weights\r\n        self.forward_weight = 1.0\r\n        self.alive_bonus = 0.1\r\n        self.energy_penalty = 0.01\r\n        self.stability_weight = 0.5\r\n    \r\n    def compute(\r\n        self,\r\n        velocity: np.ndarray,\r\n        joint_torques: np.ndarray,\r\n        orientation: np.ndarray,\r\n        is_fallen: bool\r\n    ) -> float:\r\n        """Compute reward for current state."""\r\n        if is_fallen:\r\n            return -10.0  # Large penalty for falling\r\n        \r\n        # Forward velocity reward\r\n        forward_reward = self.forward_weight * velocity[0]\r\n        \r\n        # Alive bonus\r\n        alive_reward = self.alive_bonus\r\n        \r\n        # Energy penalty (encourage efficient movement)\r\n        energy_penalty = self.energy_penalty * np.sum(joint_torques ** 2)\r\n        \r\n        # Stability reward (upright orientation)\r\n        # orientation is roll, pitch, yaw\r\n        stability_reward = self.stability_weight * (\r\n            1.0 - abs(orientation[0]) - abs(orientation[1])\r\n        )\r\n        \r\n        total_reward = forward_reward + alive_reward - energy_penalty + stability_reward\r\n        \r\n        return total_reward\r\n\r\n\r\n# Training loop (simplified)\r\ndef train_walking_policy(env, policy, num_episodes=1000):\r\n    """Train walking policy with PPO."""\r\n    optimizer = torch.optim.Adam(policy.parameters(), lr=3e-4)\r\n    reward_fn = WalkingReward()\r\n    \r\n    for episode in range(num_episodes):\r\n        obs = env.reset()\r\n        episode_reward = 0\r\n        \r\n        while True:\r\n            obs_tensor = torch.FloatTensor(obs).unsqueeze(0)\r\n            action, log_prob = policy.get_action(obs_tensor)\r\n            \r\n            next_obs, _, done, info = env.step(action.numpy().flatten())\r\n            \r\n            # Compute custom reward\r\n            reward = reward_fn.compute(\r\n                velocity=info.get(\'velocity\', np.zeros(3)),\r\n                joint_torques=info.get(\'torques\', np.zeros(12)),\r\n                orientation=info.get(\'orientation\', np.zeros(3)),\r\n                is_fallen=info.get(\'fallen\', False)\r\n            )\r\n            \r\n            episode_reward += reward\r\n            obs = next_obs\r\n            \r\n            if done:\r\n                break\r\n        \r\n        if episode % 100 == 0:\r\n            print(f"Episode {episode}: Reward = {episode_reward:.2f}")\n'})}),"\n",(0,a.jsx)(e.h3,{id:"deployment-to-real-robot",children:"Deployment to Real Robot"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n"""\r\nreal_robot_deployment.py - Deploy trained policy to real robot.\r\n"""\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import JointState, Imu\r\nfrom std_msgs.msg import Float64MultiArray\r\nimport torch\r\nimport numpy as np\r\n\r\n\r\nclass RealRobotController(Node):\r\n    """Deploy simulation-trained policy to real robot."""\r\n    \r\n    def __init__(self, policy_path: str):\r\n        super().__init__(\'real_robot_controller\')\r\n        \r\n        # Load trained policy\r\n        self.policy = torch.load(policy_path)\r\n        self.policy.eval()\r\n        \r\n        # State buffers\r\n        self.joint_positions = np.zeros(12)\r\n        self.joint_velocities = np.zeros(12)\r\n        self.imu_orientation = np.zeros(4)\r\n        self.imu_angular_vel = np.zeros(3)\r\n        \r\n        # Subscribers\r\n        self.joint_sub = self.create_subscription(\r\n            JointState, \'/joint_states\', self.joint_callback, 10\r\n        )\r\n        self.imu_sub = self.create_subscription(\r\n            Imu, \'/imu\', self.imu_callback, 10\r\n        )\r\n        \r\n        # Publisher for joint commands\r\n        self.cmd_pub = self.create_publisher(\r\n            Float64MultiArray, \'/joint_commands\', 10\r\n        )\r\n        \r\n        # Control loop at 100 Hz\r\n        self.timer = self.create_timer(0.01, self.control_loop)\r\n        \r\n        # Safety limits\r\n        self.max_joint_velocity = 2.0  # rad/s\r\n        self.max_torque = 50.0  # Nm\r\n        \r\n        self.get_logger().info(\'Real robot controller started\')\r\n    \r\n    def joint_callback(self, msg: JointState):\r\n        """Update joint state from robot."""\r\n        self.joint_positions = np.array(msg.position[:12])\r\n        self.joint_velocities = np.array(msg.velocity[:12])\r\n    \r\n    def imu_callback(self, msg: Imu):\r\n        """Update IMU state from robot."""\r\n        self.imu_orientation = np.array([\r\n            msg.orientation.x, msg.orientation.y,\r\n            msg.orientation.z, msg.orientation.w\r\n        ])\r\n        self.imu_angular_vel = np.array([\r\n            msg.angular_velocity.x,\r\n            msg.angular_velocity.y,\r\n            msg.angular_velocity.z\r\n        ])\r\n    \r\n    def build_observation(self) -> np.ndarray:\r\n        """Build observation vector matching training."""\r\n        obs = np.concatenate([\r\n            self.joint_positions,\r\n            self.joint_velocities,\r\n            self.imu_orientation,\r\n            self.imu_angular_vel\r\n        ])\r\n        return obs\r\n    \r\n    def control_loop(self):\r\n        """Main control loop."""\r\n        # Build observation\r\n        obs = self.build_observation()\r\n        obs_tensor = torch.FloatTensor(obs).unsqueeze(0)\r\n        \r\n        # Get action from policy\r\n        with torch.no_grad():\r\n            action = self.policy.get_action(obs_tensor, deterministic=True)\r\n        \r\n        action = action.numpy().flatten()\r\n        \r\n        # Apply safety limits\r\n        action = self.apply_safety_limits(action)\r\n        \r\n        # Publish command\r\n        cmd_msg = Float64MultiArray()\r\n        cmd_msg.data = action.tolist()\r\n        self.cmd_pub.publish(cmd_msg)\r\n    \r\n    def apply_safety_limits(self, action: np.ndarray) -> np.ndarray:\r\n        """Apply safety limits to actions."""\r\n        # Clip to safe range\r\n        action = np.clip(action, -1.0, 1.0)\r\n        \r\n        # Scale to actual joint limits\r\n        # (depends on robot configuration)\r\n        \r\n        return action\r\n\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    controller = RealRobotController(\'walking_policy.pt\')\r\n    rclpy.spin(controller)\r\n    controller.destroy_node()\r\n    rclpy.shutdown()\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,a.jsx)(e.h2,{id:"the-hardware-reality-warning",children:"The Hardware Reality (Warning)"}),"\n",(0,a.jsxs)(e.admonition,{title:"Sim-to-Real Failures",type:"danger",children:[(0,a.jsx)(e.p,{children:"Common failure modes when deploying to real robots:"}),(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Actuator saturation"}),": Real motors can't match simulated torques"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Sensor delays"}),": Real sensors have latency simulation ignores"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Unmodeled dynamics"}),": Cable drag, joint friction, thermal effects"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Environmental differences"}),": Floor texture, lighting, obstacles"]}),"\n"]}),(0,a.jsx)(e.p,{children:"Always start with conservative policies and gradually increase aggressiveness."})]}),"\n",(0,a.jsxs)(e.admonition,{title:"Testing Protocol",type:"warning",children:[(0,a.jsx)(e.p,{children:"Before full deployment:"}),(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Desk test"}),": Run policy with robot suspended"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Harness test"}),": Robot on safety harness"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Slow motion"}),": Run at 50% speed"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Supervised"}),": Human ready to catch/stop"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Full speed"}),": Only after all above pass"]}),"\n"]})]}),"\n",(0,a.jsx)(e.h3,{id:"debugging-sim-to-real-gaps",children:"Debugging Sim-to-Real Gaps"}),"\n",(0,a.jsxs)(e.table,{children:[(0,a.jsx)(e.thead,{children:(0,a.jsxs)(e.tr,{children:[(0,a.jsx)(e.th,{children:"Symptom"}),(0,a.jsx)(e.th,{children:"Likely Cause"}),(0,a.jsx)(e.th,{children:"Solution"})]})}),(0,a.jsxs)(e.tbody,{children:[(0,a.jsxs)(e.tr,{children:[(0,a.jsx)(e.td,{children:"Falls immediately"}),(0,a.jsx)(e.td,{children:"Friction mismatch"}),(0,a.jsx)(e.td,{children:"Increase friction randomization"})]}),(0,a.jsxs)(e.tr,{children:[(0,a.jsx)(e.td,{children:"Drifts sideways"}),(0,a.jsx)(e.td,{children:"IMU calibration"}),(0,a.jsx)(e.td,{children:"Calibrate real IMU, add bias randomization"})]}),(0,a.jsxs)(e.tr,{children:[(0,a.jsx)(e.td,{children:"Jerky motion"}),(0,a.jsx)(e.td,{children:"Action delay"}),(0,a.jsx)(e.td,{children:"Add delay randomization in training"})]}),(0,a.jsxs)(e.tr,{children:[(0,a.jsx)(e.td,{children:"Overheats motors"}),(0,a.jsx)(e.td,{children:"Torque limits"}),(0,a.jsx)(e.td,{children:"Add energy penalty, reduce action scale"})]})]})]}),"\n",(0,a.jsx)(e.h2,{id:"assessment",children:"Assessment"}),"\n",(0,a.jsx)(e.h3,{id:"recall",children:"Recall"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsx)(e.li,{children:"What is the reality gap and why does it exist?"}),"\n",(0,a.jsx)(e.li,{children:"Name three types of domain randomization."}),"\n",(0,a.jsx)(e.li,{children:"What is the purpose of adding observation noise during training?"}),"\n",(0,a.jsx)(e.li,{children:"Why should you start with conservative policies on real robots?"}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"apply",children:"Apply"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsx)(e.li,{children:"Implement domain randomization for friction and mass in a simple simulation."}),"\n",(0,a.jsx)(e.li,{children:"Design a reward function for a humanoid robot that encourages stable walking."}),"\n",(0,a.jsx)(e.li,{children:"Write a safety wrapper that limits joint velocities and torques."}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"analyze",children:"Analyze"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsx)(e.li,{children:"Compare the trade-offs between domain randomization and system identification."}),"\n",(0,a.jsx)(e.li,{children:"Why might a policy that works well in simulation with randomization still fail on a real robot?"}),"\n",(0,a.jsx)(e.li,{children:"Design an experiment to identify which simulation parameters most affect real-world performance."}),"\n"]})]})}function m(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(c,{...n})}):c(n)}},8453(n,e,r){r.d(e,{R:()=>t,x:()=>s});var i=r(6540);const a={},o=i.createContext(a);function t(n){const e=i.useContext(o);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(a):n.components||a:t(n.components),i.createElement(o.Provider,{value:e},n.children)}}}]);