"use strict";(globalThis.webpackChunkphysical_ai_textbook=globalThis.webpackChunkphysical_ai_textbook||[]).push([[617],{3219(e,n,r){r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>t,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"module3/week9-isaac-2","title":"Isaac ROS and VSLAM","description":"Hardware-accelerated perception with Isaac ROS - Visual SLAM, object detection, and navigation","source":"@site/docs/module3/week9-isaac-2.md","sourceDirName":"module3","slug":"/module3/week9-isaac-2","permalink":"/physical-ai-and-humanoid-robotics/docs/module3/week9-isaac-2","draft":false,"unlisted":false,"editUrl":"https://github.com/nimranaz148/docs/module3/week9-isaac-2.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"title":"Isaac ROS and VSLAM","description":"Hardware-accelerated perception with Isaac ROS - Visual SLAM, object detection, and navigation","keywords":["isaac-ros","vslam","perception","navigation","gpu-acceleration"]},"sidebar":"tutorialSidebar","previous":{"title":"NVIDIA Isaac Platform Overview","permalink":"/physical-ai-and-humanoid-robotics/docs/module3/week8-isaac"},"next":{"title":"Sim-to-Real Transfer","permalink":"/physical-ai-and-humanoid-robotics/docs/module3/week10-isaac-3"}}');var i=r(4848),s=r(8453);const t={sidebar_position:2,title:"Isaac ROS and VSLAM",description:"Hardware-accelerated perception with Isaac ROS - Visual SLAM, object detection, and navigation",keywords:["isaac-ros","vslam","perception","navigation","gpu-acceleration"]},o="Isaac ROS and VSLAM",l={},c=[{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"The Physics (Why)",id:"the-physics-why",level:2},{value:"The Analogy (Mental Model)",id:"the-analogy-mental-model",level:2},{value:"The Visualization (VSLAM Pipeline)",id:"the-visualization-vslam-pipeline",level:2},{value:"The Code (Implementation)",id:"the-code-implementation",level:2},{value:"Stereo Camera Configuration",id:"stereo-camera-configuration",level:3},{value:"Isaac ROS VSLAM Integration",id:"isaac-ros-vslam-integration",level:3},{value:"Launch File for Isaac ROS VSLAM",id:"launch-file-for-isaac-ros-vslam",level:3},{value:"The Hardware Reality (Warning)",id:"the-hardware-reality-warning",level:2},{value:"Handling Tracking Loss",id:"handling-tracking-loss",level:3},{value:"Assessment",id:"assessment",level:2},{value:"Recall",id:"recall",level:3},{value:"Apply",id:"apply",level:3},{value:"Analyze",id:"analyze",level:3}];function d(e){const n={admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"isaac-ros-and-vslam",children:"Isaac ROS and VSLAM"})}),"\n",(0,i.jsx)(n.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,i.jsx)(n.p,{children:"By the end of this chapter, you should be able to:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Implement Visual SLAM using Isaac ROS cuVSLAM"}),"\n",(0,i.jsx)(n.li,{children:"Configure stereo cameras for depth perception"}),"\n",(0,i.jsx)(n.li,{children:"Integrate Isaac ROS perception with Nav2 navigation"}),"\n",(0,i.jsx)(n.li,{children:"Optimize perception pipelines for real-time performance"}),"\n",(0,i.jsx)(n.li,{children:"Handle VSLAM tracking failures gracefully"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"the-physics-why",children:"The Physics (Why)"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Visual SLAM"})," (Simultaneous Localization and Mapping) solves a fundamental robotics problem: a robot must build a map of its environment while simultaneously tracking its position within that map."]}),"\n",(0,i.jsx)(n.p,{children:"The physics challenge is that cameras only capture 2D projections of a 3D world. To recover 3D structure, we need:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Stereo vision"}),": Two cameras with known separation (baseline)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Triangulation"}),": Calculate depth from pixel disparity"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Feature tracking"}),": Match visual features across frames"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Bundle adjustment"}),": Optimize camera poses and 3D points together"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"This requires processing millions of pixels per second\u2014exactly what GPUs excel at."}),"\n",(0,i.jsx)(n.h2,{id:"the-analogy-mental-model",children:"The Analogy (Mental Model)"}),"\n",(0,i.jsxs)(n.p,{children:["Think of VSLAM like ",(0,i.jsx)(n.strong,{children:"navigating a new city without GPS"}),":"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Observation"}),": You look around and notice landmarks (features)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Memory"}),": You remember where landmarks are relative to each other (map)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Localization"}),": You recognize landmarks to know where you are"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Exploration"}),": You discover new areas and add them to your mental map"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"A robot does the same thing, but with cameras instead of eyes and algorithms instead of intuition."}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Human Navigation"}),(0,i.jsx)(n.th,{children:"VSLAM Equivalent"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Recognizing landmarks"}),(0,i.jsx)(n.td,{children:"Feature detection (ORB, SIFT)"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Estimating distances"}),(0,i.jsx)(n.td,{children:"Stereo depth calculation"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Building mental map"}),(0,i.jsx)(n.td,{children:"3D point cloud construction"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:'Knowing "I\'ve been here"'}),(0,i.jsx)(n.td,{children:"Loop closure detection"})]})]})]}),"\n",(0,i.jsx)(n.h2,{id:"the-visualization-vslam-pipeline",children:"The Visualization (VSLAM Pipeline)"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-mermaid",children:'graph TB\r\n    subgraph "Sensor Input"\r\n        A[Left Camera] --\x3e C[Stereo Matcher]\r\n        B[Right Camera] --\x3e C\r\n        C --\x3e D[Depth Map]\r\n    end\r\n    \r\n    subgraph "Feature Processing"\r\n        A --\x3e E[Feature Extraction]\r\n        E --\x3e F[Feature Matching]\r\n        F --\x3e G[Motion Estimation]\r\n    end\r\n    \r\n    subgraph "SLAM Backend"\r\n        D --\x3e H[3D Point Cloud]\r\n        G --\x3e I[Pose Graph]\r\n        H --\x3e J[Map]\r\n        I --\x3e J\r\n        J --\x3e K[Loop Closure]\r\n        K --\x3e I\r\n    end\r\n    \r\n    subgraph "Output"\r\n        I --\x3e L[/visual_slam/odometry]\r\n        J --\x3e M[/visual_slam/map]\r\n    end\n'})}),"\n",(0,i.jsx)(n.h2,{id:"the-code-implementation",children:"The Code (Implementation)"}),"\n",(0,i.jsx)(n.h3,{id:"stereo-camera-configuration",children:"Stereo Camera Configuration"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n"""\r\nstereo_camera_config.py - Configure stereo cameras for Isaac ROS VSLAM.\r\n"""\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import CameraInfo, Image\r\nimport numpy as np\r\n\r\n\r\nclass StereoCameraPublisher(Node):\r\n    """Publishes stereo camera info for VSLAM."""\r\n    \r\n    def __init__(self):\r\n        super().__init__(\'stereo_camera_publisher\')\r\n        \r\n        # Camera parameters (Intel RealSense D435i style)\r\n        self.image_width = 640\r\n        self.image_height = 480\r\n        self.baseline = 0.05  # 50mm baseline\r\n        self.focal_length = 380.0  # pixels\r\n        \r\n        # Publishers for camera info\r\n        self.left_info_pub = self.create_publisher(\r\n            CameraInfo, \'/camera/left/camera_info\', 10\r\n        )\r\n        self.right_info_pub = self.create_publisher(\r\n            CameraInfo, \'/camera/right/camera_info\', 10\r\n        )\r\n        \r\n        # Publish camera info at 30 Hz\r\n        self.timer = self.create_timer(1/30, self.publish_camera_info)\r\n        \r\n        self.get_logger().info(\'Stereo camera publisher started\')\r\n    \r\n    def create_camera_info(self, is_left: bool) -> CameraInfo:\r\n        """Create CameraInfo message for stereo camera."""\r\n        info = CameraInfo()\r\n        info.header.stamp = self.get_clock().now().to_msg()\r\n        info.header.frame_id = \'camera_left\' if is_left else \'camera_right\'\r\n        \r\n        info.width = self.image_width\r\n        info.height = self.image_height\r\n        \r\n        # Intrinsic matrix K\r\n        cx = self.image_width / 2\r\n        cy = self.image_height / 2\r\n        info.k = [\r\n            self.focal_length, 0.0, cx,\r\n            0.0, self.focal_length, cy,\r\n            0.0, 0.0, 1.0\r\n        ]\r\n        \r\n        # Projection matrix P\r\n        # For right camera, Tx = -fx * baseline\r\n        tx = 0.0 if is_left else -self.focal_length * self.baseline\r\n        info.p = [\r\n            self.focal_length, 0.0, cx, tx,\r\n            0.0, self.focal_length, cy, 0.0,\r\n            0.0, 0.0, 1.0, 0.0\r\n        ]\r\n        \r\n        # Rectification matrix R (identity for rectified images)\r\n        info.r = [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]\r\n        \r\n        # Distortion (assuming rectified images)\r\n        info.distortion_model = \'plumb_bob\'\r\n        info.d = [0.0, 0.0, 0.0, 0.0, 0.0]\r\n        \r\n        return info\r\n    \r\n    def publish_camera_info(self):\r\n        """Publish camera info for both cameras."""\r\n        self.left_info_pub.publish(self.create_camera_info(is_left=True))\r\n        self.right_info_pub.publish(self.create_camera_info(is_left=False))\r\n\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = StereoCameraPublisher()\r\n    rclpy.spin(node)\r\n    node.destroy_node()\r\n    rclpy.shutdown()\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,i.jsx)(n.h3,{id:"isaac-ros-vslam-integration",children:"Isaac ROS VSLAM Integration"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n"""\r\nvslam_integration.py - Integrate Isaac ROS VSLAM with robot control.\r\n"""\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom nav_msgs.msg import Odometry, Path\r\nfrom geometry_msgs.msg import PoseStamped, Twist\r\nfrom std_msgs.msg import Bool\r\nimport numpy as np\r\nfrom enum import Enum\r\n\r\n\r\nclass TrackingState(Enum):\r\n    """VSLAM tracking states."""\r\n    INITIALIZING = 0\r\n    TRACKING = 1\r\n    LOST = 2\r\n    RELOCALIZING = 3\r\n\r\n\r\nclass VSLAMNavigator(Node):\r\n    """Navigate using Isaac ROS VSLAM for localization."""\r\n    \r\n    def __init__(self):\r\n        super().__init__(\'vslam_navigator\')\r\n        \r\n        # VSLAM state\r\n        self.tracking_state = TrackingState.INITIALIZING\r\n        self.current_pose = None\r\n        self.pose_covariance = None\r\n        self.path_history = []\r\n        \r\n        # Subscribers\r\n        self.odom_sub = self.create_subscription(\r\n            Odometry,\r\n            \'/visual_slam/tracking/odometry\',\r\n            self.odom_callback,\r\n            10\r\n        )\r\n        \r\n        self.tracking_sub = self.create_subscription(\r\n            Bool,\r\n            \'/visual_slam/status/tracking\',\r\n            self.tracking_callback,\r\n            10\r\n        )\r\n        \r\n        # Publishers\r\n        self.cmd_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\r\n        self.path_pub = self.create_publisher(Path, \'/robot_path\', 10)\r\n        \r\n        # Safety parameters\r\n        self.max_covariance = 0.1  # Stop if localization uncertain\r\n        self.lost_timeout = 2.0  # Seconds before declaring lost\r\n        self.last_good_tracking = self.get_clock().now()\r\n        \r\n        # Control loop\r\n        self.timer = self.create_timer(0.1, self.control_loop)\r\n        \r\n        self.get_logger().info(\'VSLAM Navigator started\')\r\n    \r\n    def odom_callback(self, msg: Odometry):\r\n        """Process VSLAM odometry updates."""\r\n        self.current_pose = msg.pose.pose\r\n        \r\n        # Extract position covariance (diagonal elements)\r\n        cov = msg.pose.covariance\r\n        self.pose_covariance = np.sqrt(cov[0]**2 + cov[7]**2 + cov[14]**2)\r\n        \r\n        # Update path history\r\n        pose_stamped = PoseStamped()\r\n        pose_stamped.header = msg.header\r\n        pose_stamped.pose = msg.pose.pose\r\n        self.path_history.append(pose_stamped)\r\n        \r\n        # Limit path history length\r\n        if len(self.path_history) > 1000:\r\n            self.path_history = self.path_history[-500:]\r\n        \r\n        # Publish path for visualization\r\n        path_msg = Path()\r\n        path_msg.header = msg.header\r\n        path_msg.poses = self.path_history\r\n        self.path_pub.publish(path_msg)\r\n        \r\n        # Update tracking state\r\n        if self.pose_covariance < self.max_covariance:\r\n            self.tracking_state = TrackingState.TRACKING\r\n            self.last_good_tracking = self.get_clock().now()\r\n        else:\r\n            self.tracking_state = TrackingState.RELOCALIZING\r\n    \r\n    def tracking_callback(self, msg: Bool):\r\n        """Handle tracking status updates."""\r\n        if not msg.data:\r\n            elapsed = (self.get_clock().now() - self.last_good_tracking).nanoseconds / 1e9\r\n            if elapsed > self.lost_timeout:\r\n                self.tracking_state = TrackingState.LOST\r\n                self.get_logger().error(\'VSLAM tracking lost!\')\r\n    \r\n    def control_loop(self):\r\n        """Main control loop with safety checks."""\r\n        cmd = Twist()\r\n        \r\n        if self.tracking_state == TrackingState.LOST:\r\n            # Emergency stop when tracking is lost\r\n            cmd.linear.x = 0.0\r\n            cmd.angular.z = 0.0\r\n            self.get_logger().warn(\'Stopped: VSLAM tracking lost\')\r\n            \r\n        elif self.tracking_state == TrackingState.RELOCALIZING:\r\n            # Slow rotation to help relocalization\r\n            cmd.linear.x = 0.0\r\n            cmd.angular.z = 0.2  # Slow rotation\r\n            self.get_logger().info(\'Relocalizing...\')\r\n            \r\n        elif self.tracking_state == TrackingState.TRACKING:\r\n            # Normal operation - navigation commands would go here\r\n            pass\r\n        \r\n        self.cmd_pub.publish(cmd)\r\n    \r\n    def get_position(self):\r\n        """Get current position if tracking is good."""\r\n        if self.tracking_state == TrackingState.TRACKING and self.current_pose:\r\n            return (\r\n                self.current_pose.position.x,\r\n                self.current_pose.position.y,\r\n                self.current_pose.position.z\r\n            )\r\n        return None\r\n\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = VSLAMNavigator()\r\n    rclpy.spin(node)\r\n    node.destroy_node()\r\n    rclpy.shutdown()\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,i.jsx)(n.h3,{id:"launch-file-for-isaac-ros-vslam",children:"Launch File for Isaac ROS VSLAM"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\r\n\"\"\"\r\nisaac_vslam_launch.py - Launch Isaac ROS VSLAM with configuration.\r\n\"\"\"\r\n\r\nfrom launch import LaunchDescription\r\nfrom launch_ros.actions import Node\r\nfrom launch.actions import DeclareLaunchArgument\r\nfrom launch.substitutions import LaunchConfiguration\r\n\r\n\r\ndef generate_launch_description():\r\n    \"\"\"Generate launch description for Isaac ROS VSLAM.\"\"\"\r\n    \r\n    return LaunchDescription([\r\n        # Declare arguments\r\n        DeclareLaunchArgument(\r\n            'enable_imu_fusion',\r\n            default_value='true',\r\n            description='Enable IMU fusion for better tracking'\r\n        ),\r\n        \r\n        DeclareLaunchArgument(\r\n            'enable_slam',\r\n            default_value='true',\r\n            description='Enable mapping (vs localization only)'\r\n        ),\r\n        \r\n        # Isaac ROS VSLAM node\r\n        Node(\r\n            package='isaac_ros_visual_slam',\r\n            executable='visual_slam_node',\r\n            name='visual_slam',\r\n            parameters=[{\r\n                'enable_imu_fusion': LaunchConfiguration('enable_imu_fusion'),\r\n                'enable_slam': LaunchConfiguration('enable_slam'),\r\n                'rectified_images': True,\r\n                'enable_observations_view': True,\r\n                'enable_landmarks_view': True,\r\n                'map_frame': 'map',\r\n                'odom_frame': 'odom',\r\n                'base_frame': 'base_link',\r\n            }],\r\n            remappings=[\r\n                ('stereo_camera/left/image', '/camera/left/image_raw'),\r\n                ('stereo_camera/left/camera_info', '/camera/left/camera_info'),\r\n                ('stereo_camera/right/image', '/camera/right/image_raw'),\r\n                ('stereo_camera/right/camera_info', '/camera/right/camera_info'),\r\n                ('visual_slam/imu', '/imu'),\r\n            ]\r\n        ),\r\n        \r\n        # VSLAM Navigator node\r\n        Node(\r\n            package='humanoid_navigation',\r\n            executable='vslam_navigator',\r\n            name='vslam_navigator',\r\n            output='screen'\r\n        ),\r\n    ])\n"})}),"\n",(0,i.jsx)(n.h2,{id:"the-hardware-reality-warning",children:"The Hardware Reality (Warning)"}),"\n",(0,i.jsxs)(n.admonition,{title:"VSLAM Failure Modes",type:"danger",children:[(0,i.jsx)(n.p,{children:"Visual SLAM can fail in challenging conditions:"}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Low light"}),": Insufficient features to track"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Fast motion"}),": Motion blur destroys features"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Textureless surfaces"}),": White walls, blank floors"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Dynamic scenes"}),": Moving objects confuse tracking"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Repetitive patterns"}),": Causes incorrect loop closures"]}),"\n"]}),(0,i.jsx)(n.p,{children:"Always implement fallback localization (wheel odometry, IMU dead reckoning)."})]}),"\n",(0,i.jsxs)(n.admonition,{title:"Performance Tuning",type:"warning",children:[(0,i.jsx)(n.p,{children:"Isaac ROS VSLAM performance depends on configuration:"}),(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Parameter"}),(0,i.jsx)(n.th,{children:"Low Power"}),(0,i.jsx)(n.th,{children:"Balanced"}),(0,i.jsx)(n.th,{children:"High Accuracy"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Image resolution"}),(0,i.jsx)(n.td,{children:"320x240"}),(0,i.jsx)(n.td,{children:"640x480"}),(0,i.jsx)(n.td,{children:"1280x720"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Feature count"}),(0,i.jsx)(n.td,{children:"500"}),(0,i.jsx)(n.td,{children:"1000"}),(0,i.jsx)(n.td,{children:"2000"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Keyframe rate"}),(0,i.jsx)(n.td,{children:"2 Hz"}),(0,i.jsx)(n.td,{children:"5 Hz"}),(0,i.jsx)(n.td,{children:"10 Hz"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"GPU memory"}),(0,i.jsx)(n.td,{children:"1 GB"}),(0,i.jsx)(n.td,{children:"2 GB"}),(0,i.jsx)(n.td,{children:"4 GB"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Latency"}),(0,i.jsx)(n.td,{children:"20ms"}),(0,i.jsx)(n.td,{children:"35ms"}),(0,i.jsx)(n.td,{children:"60ms"})]})]})]})]}),"\n",(0,i.jsx)(n.h3,{id:"handling-tracking-loss",children:"Handling Tracking Loss"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class TrackingRecovery:\r\n    """Strategies for recovering from VSLAM tracking loss."""\r\n    \r\n    def __init__(self):\r\n        self.recovery_strategies = [\r\n            self.slow_rotation,\r\n            self.return_to_last_known,\r\n            self.wheel_odometry_fallback,\r\n        ]\r\n    \r\n    def slow_rotation(self):\r\n        """Rotate slowly to find recognizable features."""\r\n        # Rotate 360 degrees slowly\r\n        pass\r\n    \r\n    def return_to_last_known(self):\r\n        """Navigate back to last known good position."""\r\n        # Use wheel odometry to return\r\n        pass\r\n    \r\n    def wheel_odometry_fallback(self):\r\n        """Fall back to wheel odometry until VSLAM recovers."""\r\n        # Switch to odometry-only navigation\r\n        pass\n'})}),"\n",(0,i.jsx)(n.h2,{id:"assessment",children:"Assessment"}),"\n",(0,i.jsx)(n.h3,{id:"recall",children:"Recall"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"What does VSLAM stand for and what problem does it solve?"}),"\n",(0,i.jsx)(n.li,{children:"Why is stereo vision necessary for depth perception in VSLAM?"}),"\n",(0,i.jsx)(n.li,{children:"What is loop closure and why is it important?"}),"\n",(0,i.jsx)(n.li,{children:"List three conditions that can cause VSLAM tracking to fail."}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"apply",children:"Apply"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Configure Isaac ROS VSLAM for a robot with a 10cm stereo baseline."}),"\n",(0,i.jsx)(n.li,{children:"Write a node that detects VSLAM tracking loss and triggers an emergency stop."}),"\n",(0,i.jsx)(n.li,{children:"Implement a simple recovery behavior that rotates the robot when tracking is lost."}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"analyze",children:"Analyze"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Compare the trade-offs between VSLAM and LiDAR-based SLAM for indoor navigation."}),"\n",(0,i.jsx)(n.li,{children:"Why might a humanoid robot need different VSLAM parameters when walking vs. standing still?"}),"\n",(0,i.jsx)(n.li,{children:"Design a sensor fusion strategy that combines VSLAM with wheel odometry and IMU for robust localization."}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453(e,n,r){r.d(n,{R:()=>t,x:()=>o});var a=r(6540);const i={},s=a.createContext(i);function t(e){const n=a.useContext(s);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:t(e.components),a.createElement(s.Provider,{value:n},e.children)}}}]);